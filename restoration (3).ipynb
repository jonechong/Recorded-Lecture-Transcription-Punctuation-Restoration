{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5e8c85c-86f4-4ba0-8b1c-7d846c8dc141",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.4.1.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.51.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (159 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.5/159.5 kB\u001b[0m \u001b[31m212.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.21 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (9.3.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m115.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached scikit_learn-1.4.1.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "Downloading contourpy-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (306 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.0/306.0 kB\u001b[0m \u001b[31m662.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.51.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Using cached kiwisolver-1.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "Using cached pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "Downloading scipy-1.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, pyparsing, kiwisolver, joblib, fonttools, cycler, contourpy, scikit-learn, matplotlib\n",
      "Successfully installed contourpy-1.2.1 cycler-0.12.1 fonttools-4.51.0 joblib-1.3.2 kiwisolver-1.4.5 matplotlib-3.8.4 pyparsing-3.1.2 scikit-learn-1.4.1.post1 scipy-1.13.0 threadpoolctl-3.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "741ff062-2fcc-4ff3-b59e-4dfc70ff761d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported - ready to use PyTorch 2.1.1+cu118\n"
     ]
    }
   ],
   "source": [
    "# Import PyTorch libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Other libraries we'll use\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported - ready to use PyTorch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc4977d1-11ab-42db-b569-efbe44f12216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Using cached wikipedia-1.4.0-py3-none-any.whl\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.11/site-packages (from wikipedia) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4->wikipedia) (2.5)\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c22739b5-99f1-4b6b-858a-e1b75834478c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /opt/conda/lib/python3.11/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "ename": "ConnectTimeout",
     "evalue": "HTTPConnectionPool(host='en.wikipedia.org', port=80): Max retries exceeded with url: /w/api.php?list=search&srprop=&srlimit=1&limit=1&srsearch=competitor&srinfo=suggestion&format=json&action=query (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7fc83c37a490>, 'Connection to en.wikipedia.org timed out. (connect timeout=None)'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mTimeoutError\u001b[0m: [Errno 110] Connection timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConnectTimeoutError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:496\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connection.py:395\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/http/client.py:1281\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1281\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/http/client.py:1041\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1041\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1044\u001b[0m \n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/http/client.py:979\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 979\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connection.py:243\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connection.py:212\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    215\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mConnectTimeoutError\u001b[0m: (<urllib3.connection.HTTPConnection object at 0x7fc83c37a490>, 'Connection to en.wikipedia.org timed out. (connect timeout=None)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:844\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    842\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 844\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    847\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/util/retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    514\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    517\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='en.wikipedia.org', port=80): Max retries exceeded with url: /w/api.php?list=search&srprop=&srlimit=1&limit=1&srsearch=competitor&srinfo=suggestion&format=json&action=query (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7fc83c37a490>, 'Connection to en.wikipedia.org timed out. (connect timeout=None)'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectTimeout\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m noun \u001b[38;5;129;01min\u001b[39;00m nouns[:\u001b[38;5;241m2000\u001b[39m]:  \u001b[38;5;66;03m# Limiting to the first 4000 nouns\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m         page \u001b[38;5;241m=\u001b[39m \u001b[43mwikipedia\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoun\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m         wikipedia_texts\u001b[38;5;241m.\u001b[39mappend(page\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m wikipedia\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mPageError:\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;66;03m#print(f\"No Wikipedia page found for noun: {noun}\")\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/wikipedia/wikipedia.py:270\u001b[0m, in \u001b[0;36mpage\u001b[0;34m(title, pageid, auto_suggest, redirect, preload)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m title \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m auto_suggest:\n\u001b[0;32m--> 270\u001b[0m     results, suggestion \u001b[38;5;241m=\u001b[39m \u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuggestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m       title \u001b[38;5;241m=\u001b[39m suggestion \u001b[38;5;129;01mor\u001b[39;00m results[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/wikipedia/util.py:28\u001b[0m, in \u001b[0;36mcache.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache[key]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/wikipedia/wikipedia.py:103\u001b[0m, in \u001b[0;36msearch\u001b[0;34m(query, results, suggestion)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m suggestion:\n\u001b[1;32m    101\u001b[0m   search_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrinfo\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuggestion\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 103\u001b[0m raw_results \u001b[38;5;241m=\u001b[39m \u001b[43m_wiki_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m raw_results:\n\u001b[1;32m    106\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m raw_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHTTP request timed out.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPool queue is full\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/wikipedia/wikipedia.py:737\u001b[0m, in \u001b[0;36m_wiki_request\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    734\u001b[0m   wait_time \u001b[38;5;241m=\u001b[39m (RATE_LIMIT_LAST_CALL \u001b[38;5;241m+\u001b[39m RATE_LIMIT_MIN_WAIT) \u001b[38;5;241m-\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    735\u001b[0m   time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mint\u001b[39m(wait_time\u001b[38;5;241m.\u001b[39mtotal_seconds()))\n\u001b[0;32m--> 737\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAPI_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RATE_LIMIT:\n\u001b[1;32m    740\u001b[0m   RATE_LIMIT_LAST_CALL \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/adapters.py:507\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, NewConnectionError):\n\u001b[0;32m--> 507\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ResponseError):\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RetryError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectTimeout\u001b[0m: HTTPConnectionPool(host='en.wikipedia.org', port=80): Max retries exceeded with url: /w/api.php?list=search&srprop=&srlimit=1&limit=1&srsearch=competitor&srinfo=suggestion&format=json&action=query (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7fc83c37a490>, 'Connection to en.wikipedia.org timed out. (connect timeout=None)'))"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wikipedia\n",
    "\n",
    "with open('nounlist.txt', 'r') as file:\n",
    "    nouns = [line.strip() for line in file.readlines()]\n",
    "\n",
    "wikipedia_texts = []\n",
    "for noun in nouns[:2000]:  # Limiting to the first 2000 nouns\n",
    "    try:\n",
    "        page = wikipedia.page(noun)\n",
    "        wikipedia_texts.append(page.content)\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        #print(f\"No Wikipedia page found for noun: {noun}\")\n",
    "        continue\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        #print(f\"DisambiguationError for noun: {noun}, skipping.\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d63ad4b-17e8-4be1-b5a1-1fdc6bfc03c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "728\n"
     ]
    }
   ],
   "source": [
    "print(len(wikipedia_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d8fccd4-12a8-4c3a-87dc-e0da4ae8f45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T, or t, is the twentieth letter of the Latin alphabet, used in the modern English alphabet, the alp\n",
      "17002701\n"
     ]
    }
   ],
   "source": [
    "raw_data = ' '.join(wikipedia_texts)\n",
    "print(raw_data[:100])\n",
    "print(len(raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07dce63d-fb1e-426c-bbe5-23d16c769f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "\n",
      "LINCOLN LETTERS\n",
      "\n",
      "By Abraham Lincoln\n",
      "\n",
      "\n",
      "Published by The Bibilophile Society\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NOTE\n",
      "\n",
      "The letters he\n"
     ]
    }
   ],
   "source": [
    "\n",
    "novel_texts = []\n",
    "books_directory = 'books/'\n",
    "novel_filenames = sorted(os.listdir(books_directory))[:50] \n",
    "\n",
    "for filename in novel_filenames:\n",
    "    with open(os.path.join(books_directory, filename), 'r', encoding='utf-8') as file:\n",
    "        novel_texts.append(file.read())\n",
    "\n",
    "books_data = ' '.join(novel_texts)\n",
    "print(len(novel_texts))\n",
    "print(books_data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "398a4b60-e57f-467f-a03b-9e8200a7cf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of split by spaces: 5281836\n",
      "Length of split by words: 5272600\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from random import sample\n",
    "from collections import defaultdict\n",
    "\n",
    "raw_text = raw_data + \" \" + books_data\n",
    "\n",
    "# Define replacements\n",
    "to_comma = [\":\", \";\"]\n",
    "to_period = [\"!\", \"?\"]\n",
    "to_white_space = [\"-\", \"\\n\"]\n",
    "to_one = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "\n",
    "# Create translation tables for str.translate method\n",
    "replace_comma = str.maketrans(''.join(to_comma), ',' * len(to_comma))\n",
    "replace_period = str.maketrans(''.join(to_period), '.' * len(to_period))\n",
    "replace_whitespace = str.maketrans(''.join(to_white_space), ' ' * len(to_white_space))\n",
    "replace_one = str.maketrans(''.join(to_one), '1' * len(to_one))\n",
    "\n",
    "# Function to remove unwanted characters\n",
    "def to_null(translation_table, text):\n",
    "    return text.translate(translation_table)\n",
    "\n",
    "# Function to remove unwanted characters\n",
    "def remove_unwanted_characters(text):\n",
    "    return re.sub(r'[^\\w\\s,.]', '', text)\n",
    "\n",
    "# Preprocessing function\n",
    "def to_pure_text(text):\n",
    "    # Replace specified characters with a comma or period\n",
    "    text = text.translate(replace_comma).translate(replace_period)\n",
    "    # Replace digits with '1'\n",
    "    text = text.translate(replace_one)\n",
    "    # Remove all non-English characters and non-specified punctuation\n",
    "    text = re.sub(r'[^a-zA-Z1,. ]', '', text)\n",
    "    # Substitute multiple occurrences of '1' with the word 'one'\n",
    "    text = re.sub(r'1+', 'one', text)\n",
    "    # Ensure spaces before and after periods and commas\n",
    "    text = re.sub(r'\\s*([,.])\\s*', r'\\1 ', text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to raw text\n",
    "pure_text = to_pure_text(raw_text)\n",
    "\n",
    "# Validation check (this Python version simply prints the lengths for manual checking)\n",
    "print(\"Length of split by spaces:\", len(pure_text.split()))\n",
    "print(\"Length of split by words:\", len(re.findall(r'\\w+', pure_text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9f2128e-e700-4788-8ba2-ac123f4a3e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ir. the hair on the majority of its body is grouped in clusters of one hairs. the hair surrounding its nostrils is dense to help filter particulate matter out as it digs. its tail is very thick at the base and gradually tapers. head the greatly elongated head is set on a short, thick neck, and the end of the snout bears a disc, which houses the nostrils. it contains a thin but complete zygomatic arch. the head of the aardvark contains many unique and different features. one of the most distinctive characteristics of the tubulidentata is their teeth. instead of having a pulp cavity, each tooth has a cluster of thin, hexagonal, upright, parallel tubes of vasodentin a modified form of dentine, with individual pulp canals, held together by cementum. the number of columns is dependent on the size of the tooth, with the largest having about one, one. the teeth have no enamel coating and are worn away and regrow continuously. the aardvark is born with conventional incisors and canines at the front of the jaw, which fall out and are not replaced. adult aardvarks have only cheek teeth at the back of the jaw, and have a dental formula of, one. one. one. one. one. one. one these remaining teeth are peglike and rootless and are of unique composition. the teeth consist of one upper and one lower jaw molars. the nasal area of the aardvark is another unique area, as it contains ten nasal conchae, more than any other placental mammal. the sides of the nostrils are thick with hair. the tip of the snout is highly mobile and is moved by modified mimetic muscles. the fleshy dividing tissue between its nostrils probably has sensory functions, but it is uncertain whether they are olfactory or vibratory in nature. its nose is made up of more turbinate bones than any other mammal, with between one and one, compared to dogs with one to one. with a large quantity of turbinate bones, the aardvark has more space for the moist epithelium, which is the location of the olfactory bulb. the nose co\n"
     ]
    }
   ],
   "source": [
    "print(pure_text[10000:12000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4764417e-8b6f-4136-9b46-cdcf54d0937f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26410\n"
     ]
    }
   ],
   "source": [
    "# Function to divide text into segments of 200 words\n",
    "def divide_into_segments(text, segment_length=200):\n",
    "    words = text.split()\n",
    "    return [words[i:i+segment_length] for i in range(0, len(words), segment_length)]\n",
    "\n",
    "# Function to tag words with punctuation and prepare word segments\n",
    "def tag_punctuation_and_prepare_segments(words_segment):\n",
    "    tagged_segment = [('a' if word.endswith('.') else 'b' if word.endswith(',') else 'c', re.sub(r'[,.]', '', word)) for word in words_segment]\n",
    "    words, tags = zip(*tagged_segment)  # This separates the words and tags into separate tuples\n",
    "    return list(words), list(tags)\n",
    "\n",
    "# Applying the functions\n",
    "word_segments = divide_into_segments(pure_text, 200)\n",
    "tagged_data = [tag_punctuation_and_prepare_segments(segment) for segment in word_segments if len(segment) <= 200]\n",
    "\n",
    "# Ensure all segments have 200 words, except for the possibly shorter last segment\n",
    "total_data = [(words, tags) for words, tags in tagged_data if len(words) == 200 or (len(words) < 200 and tagged_data.index((words, tags)) == len(tagged_data) - 1)]\n",
    "\n",
    "print(len(total_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38a33300-afdd-40b8-904b-9bf05849ff1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26410\n"
     ]
    }
   ],
   "source": [
    "print(len(tagged_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4825657-3200-4c56-af03-895d3f1f0c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26410\n"
     ]
    }
   ],
   "source": [
    "print(len(total_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c08fb083-703d-43ce-b737-52b24347e55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "for tags, words in tagged_data[:15]:\n",
    "    print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c910c4a-fe57-4181-a8d5-00726405fa0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['c', 'c', 'b', 'c', 'c', 'c', 'c', 'b', 'c', 'c', 'c', 'b', 'c', 'c', 'c', 'c', 'c', 'a', 'c', 'b', 'b', 'c', 'c', 'c', 'c', 'c', 'c', 'b', 'c', 'c', 'c', 'c', 'c', 'b', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'b', 'c', 'c', 'a', 'c', 'c', 'c', 'c', 'c', 'b', 'c', 'c', 'c', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'b', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'a', 'c', 'c', 'c', 'c', 'b', 'c', 'c', 'b', 'c', 'c', 'c', 'b', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'b', 'c', 'c', 'b', 'c', 'c', 'c', 'c', 'b', 'c', 'c', 'c', 'c', 'c', 'c', 'b', 'c', 'c', 'c', 'c', 'c', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'b', 'c', 'c', 'c', 'c', 'a', 'c', 'c', 'c', 'c', 'c', 'b', 'c', 'b', 'c', 'b', 'c', 'c', 'c', 'c', 'c', 'c', 'a', 'c', 'c', 'c', 'c', 'c', 'b', 'c', 'c', 'c', 'c', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'b', 'c', 'c', 'c', 'c', 'c', 'b', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'b', 'c', 'c'], ['to', 'their', 'origin', 'first', 'there', 'are', 'the', 'batchianmalays', 'probably', 'the', 'earliest', 'colonists', 'differing', 'very', 'little', 'fromthose', 'of', 'ternate', 'their', 'language', 'however', 'seems', 'to', 'have', 'more', 'ofthe', 'papuan', 'element', 'with', 'a', 'mixture', 'of', 'pure', 'malay', 'showing', 'thatthe', 'settlement', 'is', 'one', 'of', 'stragglers', 'of', 'various', 'races', 'although', 'nowsufficiently', 'homogeneous', 'then', 'there', 'are', 'the', 'orang', 'sirani', 'as', 'atternate', 'and', 'amboyna', 'many', 'of', 'these', 'have', 'the', 'portuguese', 'physiognomystrikingly', 'preserved', 'but', 'combined', 'with', 'a', 'skin', 'generally', 'darker', 'thanthe', 'malays', 'some', 'national', 'customs', 'are', 'retained', 'and', 'the', 'malay', 'whichis', 'their', 'only', 'language', 'contains', 'a', 'large', 'number', 'of', 'portuguese', 'wordsand', 'idioms', 'the', 'third', 'race', 'consists', 'of', 'the', 'galela', 'men', 'from', 'the', 'north', 'ofgilolo', 'a', 'singular', 'people', 'whom', 'i', 'have', 'already', 'described', 'and', 'the', 'fourthis', 'a', 'colony', 'from', 'tomre', 'in', 'the', 'eastern', 'peninsula', 'of', 'celebes', 'thesepeople', 'were', 'brought', 'here', 'at', 'their', 'own', 'request', 'a', 'few', 'years', 'ago', 'to', 'avoidextermination', 'by', 'another', 'tribe', 'they', 'have', 'a', 'very', 'light', 'complexion', 'opentartar', 'physiognomy', 'low', 'stature', 'and', 'a', 'language', 'of', 'the', 'bugis', 'type', 'they', 'are', 'an', 'industrious', 'agricultural', 'people', 'and', 'supply', 'the', 'town', 'withvegetables', 'they', 'make', 'a', 'good', 'deal', 'of', 'bark', 'cloth', 'similar', 'to', 'the', 'tapa', 'ofthe', 'polynesians', 'by', 'cutting', 'down', 'the', 'proper', 'trees', 'and', 'taping', 'off', 'largecylinders', 'of', 'bark', 'which', 'is'])\n",
      "(['c', 'c', 'c', 'c', 'c', 'c', 'c', 'a', 'c', 'c', 'c', 'b', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'b', 'c', 'c', 'c', 'c', 'c', 'c', 'b', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'b', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'b', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'b', 'c', 'c', 'c', 'c', 'c', 'c', 'b', 'b', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'b', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'b', 'c', 'c', 'c', 'c', 'c'], ['beaten', 'with', 'mallets', 'till', 'it', 'separates', 'fromthe', 'wood', 'it', 'is', 'then', 'soaked', 'and', 'so', 'continuously', 'and', 'regularly', 'beatenout', 'that', 'it', 'becomes', 'as', 'thin', 'and', 'as', 'tough', 'as', 'parchment', 'in', 'this', 'foam', 'itis', 'much', 'used', 'for', 'wrappers', 'for', 'clothes', 'and', 'they', 'also', 'make', 'jackets', 'of', 'it', 'sewn', 'neatly', 'together', 'and', 'stained', 'with', 'the', 'juice', 'of', 'another', 'kind', 'of', 'bark', 'which', 'gives', 'it', 'a', 'dark', 'red', 'colour', 'and', 'renders', 'it', 'nearly', 'waterproof', 'here', 'are', 'four', 'very', 'distinct', 'kinds', 'of', 'people', 'who', 'may', 'all', 'be', 'seen', 'anyday', 'in', 'and', 'about', 'the', 'town', 'of', 'batchian', 'now', 'if', 'we', 'suppose', 'a', 'travellerignorant', 'of', 'malay', 'picking', 'up', 'a', 'word', 'or', 'two', 'here', 'and', 'there', 'ofthe', 'batchian', 'language', 'and', 'noting', 'down', 'the', 'physical', 'and', 'moralpeculiarities', 'manners', 'and', 'customs', 'of', 'the', 'batchian', 'peopleforthere', 'are', 'travellers', 'who', 'do', 'all', 'this', 'in', 'fourandtwenty', 'hourswhat', 'anaccurate', 'and', 'instructive', 'chapter', 'we', 'should', 'have', 'what', 'transitions', 'wouldbe', 'pointed', 'out', 'what', 'theories', 'of', 'the', 'origin', 'of', 'races', 'would', 'be', 'developedwhile', 'the', 'next', 'traveller', 'might', 'flatly', 'contradict', 'every', 'statement', 'andarrive', 'at', 'exactly', 'opposite', 'conclusions', 'soon', 'after', 'i', 'arrived', 'here', 'the', 'dutch', 'government', 'introduced', 'a', 'new', 'coppercoinage', 'of', 'cents', 'instead', 'of', 'doits', 'the', 'oneth', 'instead', 'of', 'the', 'oneth', 'partof', 'a', 'guilder', 'and', 'all', 'the', 'old', 'coins'])\n"
     ]
    }
   ],
   "source": [
    "print(tagged_data[23139])\n",
    "print(tagged_data[23140])\n",
    "#print(total_data[0])\n",
    "\n",
    "#print(len(word_segments[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2681a560-cc17-4a17-96bf-4b05cbf984a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten all words from the training set\n",
    "all_words = [word for _, tokens in total_data for word in tokens]\n",
    "\n",
    "# Build vocabulary\n",
    "vocab_count = Counter(all_words)\n",
    "word_to_idx = {word: idx + 1 for idx, (word, _) in enumerate(vocab_count.items())}  # +1 because 0 is often reserved for padding\n",
    "word_to_idx[\"<UNK>\"] = 0  # Adding a token for unknown words\n",
    "\n",
    "vocab_size = len(word_to_idx)  # The size of your vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "051cbe32-9734-4a21-97c5-5fc2e83fa0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66100\n"
     ]
    }
   ],
   "source": [
    "print(word_to_idx['imprints'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6a6128a-adde-4798-80ee-a458dd3a8fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(tokenized_text, word_to_idx):\n",
    "    return [word_to_idx.get(word, word_to_idx[\"<UNK>\"]) for word in tokenized_text]\n",
    "\n",
    "# Convert the training set\n",
    "total_data_indices = [([tag for tag in tags], text_to_sequence(tokens, word_to_idx)) for tags, tokens in total_data]\n",
    "\n",
    "tag_to_idx = {'a': 0, 'b': 1, 'c': 2}  # Example mapping of tags to indices\n",
    "\n",
    "# Convert tags to indices\n",
    "total_data_prepared = [([tag_to_idx[tag] for tag in tags], tokens) for tags, tokens in total_data_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b618579-1a71-4d36-9515-d21ef73aa102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2], [1, 2, 1, 3, 4, 5, 6, 7, 4, 8, 9, 10, 11, 4, 12, 13, 9, 4, 14, 7, 15, 16, 17, 18, 19, 20, 21, 22, 23, 11, 13, 3, 24, 25, 26, 27, 28, 3, 29, 30, 4, 31, 32, 7, 4, 33, 19, 34, 35, 36, 19, 37, 32, 38, 32, 19, 39, 1, 40, 4, 41, 6, 42, 11, 13, 28, 3, 43, 44, 10, 45, 46, 4, 47, 48, 49, 50, 51, 28, 52, 53, 11, 4, 54, 55, 9, 28, 3, 4, 43, 44, 10, 56, 19, 4, 57, 44, 10, 6, 11, 58, 59, 60, 32, 61, 4, 62, 6, 7, 4, 16, 31, 19, 37, 14, 4, 51, 63, 7, 31, 32, 41, 9, 1, 42, 64, 65, 19, 8, 1, 66, 67, 68, 69, 70, 1, 11, 71, 7, 72, 19, 28, 66, 52, 73, 22, 74, 75, 76, 11, 43, 7, 72, 14, 77, 11, 78, 79, 13, 11, 13, 1, 80, 53, 4, 47, 48, 49, 54, 55, 9, 19, 81, 1, 82, 11, 83, 24, 2, 84, 85, 86, 87, 88, 4, 89, 7, 90, 2, 91, 92, 93, 4, 94, 95, 85, 96, 45, 4, 51])\n"
     ]
    }
   ],
   "source": [
    "print(total_data_prepared[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02496b6f-e5bf-46c5-8662-7803a35c53a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "# Calculate the number of data points for each set based on the given ratios\n",
    "total_length = len(total_data)\n",
    "train_ratio = 0.75\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10  # Assumes the remaining 10% of the data after allocating training and validation sets\n",
    "\n",
    "# Calculate split indices\n",
    "train_split = int(total_length * train_ratio)\n",
    "validation_split = train_split + int(total_length * validation_ratio)\n",
    "\n",
    "# Generate a list of indices and shuffle them\n",
    "indices = list(range(total_length))\n",
    "shuffle(indices)\n",
    "\n",
    "# Partition the data into training, validation, and test sets\n",
    "training_set = [total_data_prepared[i] for i in indices[:train_split]]\n",
    "validation_set = [total_data_prepared[i] for i in indices[train_split:validation_split]]\n",
    "test_set = [total_data_prepared[i] for i in indices[validation_split:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "784bf618-098b-4af7-bb32-edb5704260c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19807\n",
      "([2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2], [170721, 2443, 1335, 650, 26364, 19, 11833, 599, 19, 402, 171757, 4647, 721, 133611, 82, 4207, 5549, 345, 124, 342, 5139, 180864, 1350, 1015, 286, 100, 4207, 608, 12816, 2, 6433, 4207, 1545, 98694, 11, 4, 16549, 7, 20829, 22673, 180865, 86161, 30150, 19, 180866, 2034, 7, 4, 7833, 20272, 555, 4, 12302, 988, 180867, 282, 7, 4, 1901, 2759, 1350, 86, 1317, 291, 180868, 144, 124, 887, 4207, 475, 1697, 292, 4, 2532, 20797, 619, 14420, 180869, 120, 7899, 6433, 30, 4, 55214, 11634, 4207, 2268, 15669, 102635, 5975, 667, 5908, 5406, 102, 50, 17444, 120, 180870, 7651, 19, 1110, 4593, 86, 286, 7, 12909, 4, 180871, 43, 18678, 363, 1214, 3, 1020, 5442, 7, 1244, 7, 629, 2, 17768, 7, 1514, 45, 615, 112, 28, 19, 100, 4207, 2268, 286, 174946, 4, 988, 19, 3074, 4, 170879, 7, 4, 28325, 180872, 556, 3989, 1844, 555, 4, 2733, 188, 2321, 7, 599, 164719, 407, 800, 556, 801, 5023, 4207, 11143, 343, 10357, 97944, 470, 19, 144277, 3358, 343, 928, 45, 4650, 5144, 176804, 10758, 3950, 6684, 19, 805, 45, 120, 9189, 8155, 7, 1514, 180873, 34691, 7338, 1658, 421, 19, 5321, 282, 4, 6331, 7, 1244, 180874, 80, 6666])\n"
     ]
    }
   ],
   "source": [
    "print(len(training_set))\n",
    "print(training_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c223a9e6-acf9-4180-a132-5673c3cdb3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    words, tags = zip(*batch)  # Unzip the batch\n",
    "    \n",
    "    # Directly pad sequences without converting if they're already tensors\n",
    "    words_padded = pad_sequence(words, batch_first=True, padding_value=0)  # Assuming 0 is your padding value\n",
    "    tags_padded = pad_sequence(tags, batch_first=True, padding_value=0)  # Ensure the padding value for tags is correct\n",
    "    \n",
    "    return words_padded, tags_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f591ca69-320e-449d-aaa6-0ca453268017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class PunctuationDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tags, tokens = self.data[idx]\n",
    "        return torch.tensor(tokens, dtype=torch.long), torch.tensor(tags, dtype=torch.long)\n",
    "\n",
    "batch_size = 32  # Example batch size\n",
    "\n",
    "train_dataset = PunctuationDataset(training_set)\n",
    "val_dataset = PunctuationDataset(validation_set)\n",
    "test_dataset = PunctuationDataset(test_set)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ff75bc8-1263-4e50-b43a-e4e792e55649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64 torch.int64\n",
      "torch.Size([32, 200]) torch.Size([32, 200])\n"
     ]
    }
   ],
   "source": [
    "for labels, inputs in train_dataloader:\n",
    "    print(inputs.dtype, labels.dtype)  # Should output something like torch.long, torch.long\n",
    "    print(inputs.shape, labels.shape)  # Check shapes for consistency\n",
    "    break  # To only inspect the first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "777db88f-dc01-4c77-a13a-e668abd7bb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229260\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_to_idx) \n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50fbf113-f4f9-40fa-b5bb-79d86685daf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(path):\n",
    "    embeddings_dict = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = torch.tensor([float(val) for val in values[1:]], dtype=torch.float)\n",
    "            embeddings_dict[word] = vector\n",
    "    return embeddings_dict\n",
    "\n",
    "glove_path = 'glove.6B.100d.txt'\n",
    "glove_embeddings = load_glove_embeddings(glove_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da15b2e1-b8ee-45ac-92b6-6ea620bde126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229260\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100  # GloVe 100-dimensional vectors\n",
    "print(vocab_size)\n",
    "\n",
    "# Initialize the embedding matrix\n",
    "embedding_matrix = torch.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# Assuming you have a 'word_to_idx' dictionary mapping each word to an index\n",
    "for word, idx in word_to_idx.items():\n",
    "    if idx >= vocab_size:\n",
    "        continue\n",
    "    if word in glove_embeddings:\n",
    "        embedding_matrix[idx] = glove_embeddings[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = torch.randn(embedding_dim)  # Random embedding for unknown words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24833fba-71e1-4fea-89e2-e284e53ff650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4724,  0.2270, -0.4743,  0.1719,  0.9573,  0.2646,  1.3225, -0.3482,\n",
      "         1.7004, -0.8220,  1.1364, -0.6759, -0.3743, -0.1851, -2.4039, -0.5341,\n",
      "        -0.2271,  1.8260, -0.6606, -0.6483,  0.6405,  0.3656, -0.3934,  0.5835,\n",
      "        -0.0914, -0.3217, -0.8237, -2.3008,  1.5760, -0.7516, -0.5039, -0.3551,\n",
      "        -0.1450,  0.3043,  0.1004,  2.2663,  1.1313, -0.7747,  0.2311, -1.1938,\n",
      "         0.5737,  0.0850,  0.9715, -1.2466,  1.2298,  0.7593,  0.6419, -2.5115,\n",
      "         0.2672, -0.8378,  0.7068, -1.3269, -1.5178,  0.4476,  0.1263,  0.2504,\n",
      "        -1.6747, -0.1561,  0.6752,  0.1699, -0.0925, -0.3503,  0.5907,  0.6047,\n",
      "        -0.9527, -0.7762,  2.0054, -2.1857,  1.0045, -0.2163, -1.8431,  0.8421,\n",
      "        -1.3256,  1.1123,  1.0672, -1.0580, -0.4344, -0.4151, -1.0957, -2.1345,\n",
      "        -0.7316, -1.8595, -0.6591,  1.0850, -0.8278, -0.3084,  0.1107, -0.6854,\n",
      "        -0.0681,  1.2706,  0.8094, -0.7294,  0.0894, -0.8424,  0.0762,  0.8459,\n",
      "        -1.2592, -1.3931,  0.5867,  0.1123])\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa1189c4-8434-41bb-984e-b60e1543f53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PunctuationRestorationModel(\n",
      "  (embedding): Embedding(229260, 100)\n",
      "  (lstm1): LSTM(100, 100, batch_first=True)\n",
      "  (bidirectional_lstm_gru): LSTM(100, 40, batch_first=True, bidirectional=True)\n",
      "  (bidirectional_lstm2): LSTM(80, 20, batch_first=True, bidirectional=True)\n",
      "  (lstm2): LSTM(40, 10, batch_first=True)\n",
      "  (fc): Linear(in_features=10, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PunctuationRestorationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, pretrained_embeddings):\n",
    "        super(PunctuationRestorationModel, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(pretrained_embeddings)  # Initialize with GloVe\n",
    "        \n",
    "        # LSTM layer followed by Bidirectional LSTM and GRU layers\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, 100, batch_first=True)\n",
    "        \n",
    "        # Bidirectional LSTM layer\n",
    "        self.bidirectional_lstm_gru = nn.LSTM(100, 40, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # A second Bidirectional LSTM layer, but this time we'll just simulate the GRU behavior with LSTM parameters\n",
    "        self.bidirectional_lstm2 = nn.LSTM(80, 20, batch_first=True, bidirectional=True)  # Input dim is doubled due to bidirection\n",
    "        \n",
    "        # Final LSTM layer\n",
    "        self.lstm2 = nn.LSTM(40, 10, batch_first=True)  # Adjusting the input size according to the previous bidirectional output\n",
    "        \n",
    "        # Linear layer that maps the LSTM layer outputs to 3 classes\n",
    "        self.fc = nn.Linear(10, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding layer\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # First LSTM layer\n",
    "        lstm_out, _ = self.lstm1(embedded)\n",
    "        \n",
    "        # Bidirectional LSTM/GRU layer\n",
    "        bidirectional_out, _ = self.bidirectional_lstm_gru(lstm_out)\n",
    "        \n",
    "        # Simulating the second bidirectional layer (GRU behavior with LSTM for demonstration)\n",
    "        bidirectional_out2, _ = self.bidirectional_lstm2(bidirectional_out)\n",
    "        \n",
    "        # Final LSTM layer\n",
    "        final_out, _ = self.lstm2(bidirectional_out2)\n",
    "        \n",
    "        # Pass the output through the linear layer and apply softmax\n",
    "        # Note: Softmax is not applied here if using nn.CrossEntropyLoss() as it includes softmax\n",
    "        out = self.fc(final_out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Assuming the vocabulary size and embedding dimension are predefined\n",
    "output_dim = 3  # For 'a', 'b', 'c' classes\n",
    "\n",
    "model = PunctuationRestorationModel(vocab_size, embedding_dim, output_dim, embedding_matrix)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97b35782-d49f-4709-9ad6-0a245203c6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Learning rate may need adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd7fca15-d7e7-440c-a0f4-6dd8027f62ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.43779333429305733\n",
      "Validation Loss: 0.3351297008414422\n",
      "Epoch 2, Loss: 0.3066768998668344\n",
      "Validation Loss: 0.2887669945916822\n",
      "Epoch 3, Loss: 0.266725469025541\n",
      "Validation Loss: 0.2705959228257979\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3  # Number of epochs to train for\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in train_dataloader:\n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = loss = criterion(outputs.view(-1, 3), labels.view(-1))\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_dataloader)}')\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():  # No need to track gradients for validation\n",
    "        for inputs, labels in val_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            loss = loss = criterion(outputs.view(-1, 3), labels.view(-1))\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    print(f'Validation Loss: {val_loss / len(val_dataloader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8d11358-d9b6-49a2-9fc1-cbad986a2902",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = 'model/model_epoch_3_loss_0_266_val_loss_0_27.pth'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08618f0d-3775-4f15-96ab-861ada7b4eed",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Collect all predictions and true labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        outputs = model(inputs)\n",
    "        all_preds.extend(torch.argmax(outputs, dim=2).flatten().tolist())  # Assuming your model outputs raw logits\n",
    "        all_labels.extend(labels.flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92fad990-dfea-4a83-889b-e552d1f548fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision (Period): 0.5938437560880576, Recall (Period): 0.3391863622201451, F1 (Period): 0.4317624895536764\n",
      "Precision (Comma): 0.9365478347411131, Recall (Comma): 0.9678465707184251, F1 (Comma): 0.9519400052524585\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Convert tags: 'a' -> 1, 'b' -> 2, 'c' -> 0\n",
    "realTag = [1 if tag == 1 else 2 if tag == 2 else 0 for tag in all_labels]\n",
    "resultTag = [1 if pred == 1 else 2 if pred == 2 else 0 for pred in all_preds]\n",
    "\n",
    "# Calculate metrics\n",
    "prec_period = precision_score(realTag, resultTag, labels=[1], average='macro')\n",
    "rec_period = recall_score(realTag, resultTag, labels=[1], average='macro')\n",
    "f1_period = f1_score(realTag, resultTag, labels=[1], average='macro')\n",
    "\n",
    "prec_comma = precision_score(realTag, resultTag, labels=[2], average='macro')\n",
    "rec_comma = recall_score(realTag, resultTag, labels=[2], average='macro')\n",
    "f1_comma = f1_score(realTag, resultTag, labels=[2], average='macro')\n",
    "\n",
    "print(f\"Precision (Period): {prec_period}, Recall (Period): {rec_period}, F1 (Period): {f1_period}\")\n",
    "print(f\"Precision (Comma): {prec_comma}, Recall (Comma): {rec_comma}, F1 (Comma): {f1_comma}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33105883-00e5-4efa-a049-d0ff75680e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, word_to_idx):\n",
    "    tokens = text.lower().split()\n",
    "    indices = [word_to_idx.get(token, word_to_idx[\"<UNK>\"]) for token in tokens]\n",
    "    return torch.tensor(indices, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5935fc2-f5ca-4493-adbb-ce7123e8074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model, word_to_idx, idx_to_tag):\n",
    "    model.eval() \n",
    "    with torch.no_grad():\n",
    "        indices = preprocess_text(text, word_to_idx)\n",
    "        outputs = model(indices)\n",
    "        predictions = torch.argmax(outputs, dim=2) \n",
    "    predicted_tags = [idx_to_tag[idx.item()] for idx in predictions.squeeze()] \n",
    "    return predicted_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dba76f02-faeb-40ff-a892-0d56a57ee581",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_tag = {0: 'a', 1: 'b', 2: 'c'}  \n",
    "\n",
    "def apply_predicted_tags(text, predicted_tags):\n",
    "    tokens = text.lower().split()\n",
    "    final_text = \"\"\n",
    "    for token, tag in zip(tokens, predicted_tags):\n",
    "        if tag == 'a':  # 'a' is for periods\n",
    "            final_text += token + \". \"\n",
    "        elif tag == 'b':  # 'b' is for commas\n",
    "            final_text += token + \", \"\n",
    "        else:  # 'c' or other tags mean no punctuation\n",
    "            final_text += token + \" \"\n",
    "    return final_text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb522e08-bb50-43f1-8df1-d675d48bf288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capitalize_after_fullstop(text):\n",
    "    # Split the text into sentences\n",
    "    sentences = text.split('. ')\n",
    "    # Capitalize the first character of each sentence and strip leading/trailing spaces\n",
    "    capitalized_sentences = [sentence.strip().capitalize() for sentence in sentences]\n",
    "    # Join the sentences back together\n",
    "    capitalized_text = '. '.join(capitalized_sentences)\n",
    "    return capitalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da4cf19c-912a-49b7-96fa-2d92d4657278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_apply_tags(text, model, word_to_idx, idx_to_tag):\n",
    "    predicted_tags = predict(text, model, word_to_idx, idx_to_tag)  # 'predict' function as defined in the previous response\n",
    "    final_text = apply_predicted_tags(text, predicted_tags)\n",
    "    capitalized_text = capitalize_after_fullstop(final_text)\n",
    "    return capitalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cd71a8be-d137-47f0-b68a-855ceaec2979",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_text = \"Bryant started playing basketball when he was three, and the Lakers were his favorite team when he was growing up. When he was six, his father retired from \\\n",
    "the NBA and moved his family to Rieti in Italy to continue playing professional basketball. After two years, they moved first to Reggio Calabria, then to Pistoia and \\\n",
    "Reggio Emilia. Kobe became accustomed to his new lifestyle and learned to speak fluent Italian. He was especially fond of Reggio Emilia, which he considered a loving \\\n",
    "place and where some of his best childhood memories were made. He began to play basketball seriously while living in Reggio Emilia. His grandfather mailed him videos \\\n",
    "of NBA games for him to study. Another source of inspiration was animated European films about sports, from which he learned more about basketball. From 1987 to 1989, \\\n",
    "his father played for Olimpia Basket Pistoia where he paired with former Detroit Pistons player Leon Douglas. Kobe worked at the games as a ball and mop boy and practiced \\\n",
    "shooting at halftime. \\\n",
    "Bryant was sidelined for six weeks prior to the start of the 1999–2000 season due to a hand injury that he had incurred during a preseason game against the \\\n",
    "Washington Wizards. When Bryant was back and playing over 38 minutes a game, he had an increase in all statistical categories during the 1999–2000 season. This included \\\n",
    "leading the team in assists per game and steals per game. The duo of O'Neal and Bryant backed with a strong bench led to the Lakers winning 67 games, tied for fifth-most \\\n",
    "in NBA history. This followed with O'Neal winning the MVP and Bryant being named to the All-NBA Second Team and All-NBA Defensive Team for the first time in his career\\\n",
    "(the youngest player to receive All-Defensive honors). While playing second fiddle to O'Neal in the playoffs, Bryant had some clutch performances, including a 25-point, \\\n",
    "11-rebound, 7-assist, 4-block game in Game 7 of the Western Conference Finals against the Portland Trail Blazers. He also threw an alley-oop pass to O'Neal to clinch the \\\n",
    "game and the series. In the 2000 Finals, against the Indiana Pacers, Bryant injured his ankle in the second quarter of Game 2 after landing on the Pacers' Jalen Rose's \\\n",
    "foot. Rose later admitted he placed his foot under Bryant intentionally. Bryant did not return to the game, and he also missed Game 3 due to the injury. In Game 4, Bryant \\\n",
    "scored 22 points in the second half and led the team to an OT victory as O'Neal fouled out of the game. Bryant scored the winning shot to put the Lakers ahead 120–118. \\\n",
    "With a 116–111 victory in Game 6, the Lakers won their first championship since 1988.\\\n",
    "On My World 2.0, Bieber's voice was noted to be deeper than it was in his debut EP, due to puberty. In April 2010, the singer remarked regarding his vocals: 'It cracks. \\\n",
    "Like every teenage boy, I'm dealing with it and I have the best vocal coach in the world ... Some of the notes I hit on 'Baby' I can't hit any more. We have to lower the \\\n",
    "key when I sing live.' Bieber guest-starred in the season premiere of the CBS American crime drama CSI: Crime Scene Investigation, which aired on September 23, 2010. He \\\n",
    "played a 'troubled teen who is faced with a difficult decision regarding his only brother', who is also a serial bomber. Bieber was also in a subsequent episode of the \\\n",
    "series, which aired on February 17, 2011, in which his character is killed. Bieber performed a medley of his singles 'U Smile', 'Baby', and 'Somebody to Love', and briefly \\\n",
    "played the drums, at the 2010 MTV Video Music Awards on September 12, 2010. Bieber announced in October 2010 that he would be releasing an acoustic album, called My Worlds \\\n",
    "Acoustic. It was released on November 26, 2010, in the United States and featured acoustic versions of songs from his previous albums, and accompanied the release of a \\\n",
    "new song titled 'Pray'. In October 2010, Bieber released his first book, Justin Bieber: First Step 2 Forever: My Story, an autobiography with text from Bieber and \\\n",
    "photographs from Robert Caplin.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1b932876-8366-4af7-ab55-c8df22f13549",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"bryant started playing basketball when he was three and the lakers were his favorite team when he was growing up when he was six his father retired from the nba \\\n",
    "and moved his family to rieti in italy to continue playing professional basketball after two years they moved first to reggio calabria then to pistoia and reggio emilia \\\n",
    "kobe became accustomed to his new lifestyle and learned to speak fluent italian he was especially fond of reggio emilia which he considered a loving place and where some \\\n",
    "of his best childhood memories were made he began to play basketball seriously while living in reggio emilia his grandfather mailed him videos of nba games for him to \\\n",
    "study another source of inspiration was animated european films about sports from which he learned more about basketball from 1987 to 1989 his father played for olimpia \\\n",
    "basket pistoia where he paired with former detroit pistons player leon douglas kobe worked at the games as a ball and mop boy and practiced shooting at halftime bryant was \\\n",
    "sidelined for six weeks prior to the start of the 1999–2000 season due to a hand injury that he had incurred during a preseason game against the washington wizards when \\\n",
    "bryant was back and playing over 38 minutes a game he had an increase in all statistical categories during the 1999–2000 season this included leading the team in assists \\\n",
    "per game and steals per game the duo of o'neal and bryant backed with a strong bench led to the lakers winning 67 games tied for fifth-most in nba history this followed \\\n",
    "with o'neal winning the mvp and bryant being named to the all-nba second team and all-nba defensive team for the first time in his career (the youngest player to receive \\\n",
    "all-defensive honors) while playing second fiddle to o'neal in the playoffs bryant had some clutch performances including a 25-point 11-rebound 7-assist 4-block game in \\\n",
    "game 7 of the western conference finals against the portland trail blazers he also threw an alley-oop pass to o'neal to clinch the game and the series in the 2000 finals \\\n",
    "against the indiana pacers bryant injured his ankle in the second quarter of game 2 after landing on the pacers' jalen rose's foot rose later admitted he placed his foot \\\n",
    "under bryant intentionally bryant did not return to the game and he also missed game 3 due to the injury in game 4 bryant scored 22 points in the second half and led the \\\n",
    "team to an ot victory as o'neal fouled out of the game bryant scored the winning shot to put the lakers ahead 120–118 with a 116–111 victory in game 6 the lakers won \\\n",
    "their first championship since 1988 \\\n",
    "on my world 20 biebers voice was noted to be deeper than it was in his debut ep due to puberty in april 2010 the singer remarked regarding his vocals 'it cracks like every \\\n",
    "teenage boy im dealing with it and i have the best vocal coach in the world  some of the notes i hit on 'baby' i cant hit any more we have to lower the key when i sing \\\n",
    "live' bieber guest-starred in the season premiere of the cbs american crime drama csi crime scene investigation which aired on september 23 2010 he played a 'troubled \\\n",
    "teen who is faced with a difficult decision regarding his only brother' who is also a serial bomber bieber was also in a subsequent episode of the series which aired on \\\n",
    "february 17 2011 in which his character is killed bieber performed a medley of his singles 'u smile' 'baby' and 'somebody to love' and briefly played the drums at the \\\n",
    "2010 mtv video music awards on september 12 2010 bieber announced in october 2010 that he would be releasing an acoustic album called my worlds acoustic it was released \\\n",
    "on november 26 2010 in the united states and featured acoustic versions of songs from his previous albums and accompanied the release of a new song titled 'pray' in \\\n",
    "october 2010 bieber released his first book justin bieber first step 2 forever my story an autobiography with text from bieber and photographs from robert caplin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d937e8bf-e570-4511-8147-9b219c6b9b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bryant started playing basketball when he was three, and the Lakers were his favorite team when he was growing up. When he was six, his father retired from the NBA and moved his family to Rieti in Italy to continue playing professional basketball. After two years, they moved first to Reggio Calabria, then to Pistoia and Reggio Emilia. Kobe became accustomed to his new lifestyle and learned to speak fluent Italian. He was especially fond of Reggio Emilia, which he considered a loving place and where some of his best childhood memories were made. He began to play basketball seriously while living in Reggio Emilia. His grandfather mailed him videos of NBA games for him to study. Another source of inspiration was animated European films about sports, from which he learned more about basketball. From 1987 to 1989, his father played for Olimpia Basket Pistoia where he paired with former Detroit Pistons player Leon Douglas. Kobe worked at the games as a ball and mop boy and practiced shooting at halftime. Bryant was sidelined for six weeks prior to the start of the 1999–2000 season due to a hand injury that he had incurred during a preseason game against the Washington Wizards. When Bryant was back and playing over 38 minutes a game, he had an increase in all statistical categories during the 1999–2000 season. This included leading the team in assists per game and steals per game. The duo of O'Neal and Bryant backed with a strong bench led to the Lakers winning 67 games, tied for fifth-most in NBA history. This followed with O'Neal winning the MVP and Bryant being named to the All-NBA Second Team and All-NBA Defensive Team for the first time in his career(the youngest player to receive All-Defensive honors). While playing second fiddle to O'Neal in the playoffs, Bryant had some clutch performances, including a 25-point, 11-rebound, 7-assist, 4-block game in Game 7 of the Western Conference Finals against the Portland Trail Blazers. He also threw an alley-oop pass to O'Neal to clinch the game and the series. In the 2000 Finals, against the Indiana Pacers, Bryant injured his ankle in the second quarter of Game 2 after landing on the Pacers' Jalen Rose's foot. Rose later admitted he placed his foot under Bryant intentionally. Bryant did not return to the game, and he also missed Game 3 due to the injury. In Game 4, Bryant scored 22 points in the second half and led the team to an OT victory as O'Neal fouled out of the game. Bryant scored the winning shot to put the Lakers ahead 120–118. With a 116–111 victory in Game 6, the Lakers won their first championship since 1988.On My World 2.0, Bieber's voice was noted to be deeper than it was in his debut EP, due to puberty. In April 2010, the singer remarked regarding his vocals: 'It cracks. Like every teenage boy, I'm dealing with it and I have the best vocal coach in the world ... Some of the notes I hit on 'Baby' I can't hit any more. We have to lower the key when I sing live.' Bieber guest-starred in the season premiere of the CBS American crime drama CSI: Crime Scene Investigation, which aired on September 23, 2010. He played a 'troubled teen who is faced with a difficult decision regarding his only brother', who is also a serial bomber. Bieber was also in a subsequent episode of the series, which aired on February 17, 2011, in which his character is killed. Bieber performed a medley of his singles 'U Smile', 'Baby', and 'Somebody to Love', and briefly played the drums, at the 2010 MTV Video Music Awards on September 12, 2010. Bieber announced in October 2010 that he would be releasing an acoustic album, called My Worlds Acoustic. It was released on November 26, 2010, in the United States and featured acoustic versions of songs from his previous albums, and accompanied the release of a new song titled 'Pray'. In October 2010, Bieber released his first book, Justin Bieber: First Step 2 Forever: My Story, an autobiography with text from Bieber and photographs from Robert Caplin.\n",
      "\n",
      "Bryant started playing basketball when he was three, and the lakers were his favorite team. When he was growing up when he was six. His father retired from the nba and moved his family to rieti in italy to continue playing professional basketball after two years. They moved first to reggio calabria, then to pistoia and reggio. Emilia. Kobe became accustomed to his new lifestyle and learned to speak fluent italian. He was especially fond of reggio emilia, which he considered a loving place, and where some of his best childhood memories were made. He began to play basketball seriously, while living in reggio emilia. His grandfather mailed him videos of nba games for him to study. Another source of inspiration was animated european films about sports from which he learned more about basketball from 1987 to 1989. His father played for olimpia, basket pistoia, where he paired with former detroit pistons player. Leon douglas kobe worked at the games as a ball and mop boy and practiced shooting at halftime bryant was sidelined for six weeks prior to the start of the 1999–2000 season due to a hand injury that he had incurred during a preseason game against the washington wizards when bryant was back and playing over 38 minutes a game he had an increase in all statistical categories during the 1999–2000 season. This included leading the team in assists per game and steals per game. The duo of o'neal and bryant backed with a strong bench led to the lakers winning 67 games tied for fifth-most. In nba. History. This followed with o'neal winning the mvp and bryant being named to the all-nba. Second team and all-nba. Defensive team for the first time in his career (the, youngest player to receive all-defensive honors), while playing second fiddle to o'neal. In the playoffs bryant had some clutch performances, including a 25-point, 11-rebound. 7-assist. 4-block. Game in game, 7 of the western conference finals against the portland trail. Blazers. He also threw an alley-oop pass to o'neal to clinch the game and the series in the 2000 finals against the indiana pacers bryant injured his ankle in the second quarter of game 2 after landing on the pacers', jalen. Rose's foot rose later admitted he placed his foot under bryant, intentionally bryant did not return to the game, and he also missed game 3 due to the injury in game, 4, bryant scored 22 points in the second half, and led the team to an ot victory as o'neal fouled out of the game bryant scored the winning shot to put the lakers ahead. 120–118 with a 116–111 victory in game. 6, the lakers won their first championship since 1988 on my world 20. Biebers. Voice was noted to be deeper than it was in his debut ep. Due to puberty. In april 2010. The singer remarked regarding his vocals 'it cracks like every teenage boy, im dealing with it, and i have the best vocal coach in the world. Some of the notes i hit on 'baby'. I cant hit any more we have to lower the key when i sing live', bieber. Guest-starred in the season premiere of the cbs american crime drama csi crime scene investigation which aired on september 23, 2010. He played a 'troubled, teen who is faced with a difficult decision regarding his only brother', who is also a serial bomber bieber was also in a subsequent episode of the series which aired on february 17, 2011, in which his character is killed bieber performed a medley of his singles 'u. Smile'. 'baby', and 'somebody to love', and briefly played the drums at the 2010. Mtv video music awards on september 12, 2010. Bieber, announced in october 2010, that he would be releasing an acoustic album called my worlds acoustic. It was released on november 26, 2010, in the united states and featured acoustic versions of songs from his previous albums and accompanied the release of a new song titled 'pray'. In october 2010, bieber. Released his first book justin bieber. First step 2 forever my story an autobiography with text from bieber and photographs from robert caplin.\n"
     ]
    }
   ],
   "source": [
    "final_text_with_tags = predict_and_apply_tags(text, model, word_to_idx, idx_to_tag)\n",
    "print(ori_text)\n",
    "print(\"\")\n",
    "print(final_text_with_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9e5d4c15-54b2-4a3b-8fe5-d96f88b0d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_text_2 = \"In the last section, we examined some early aspects of memory. In this section, what we’re going to do is discuss some factors that influence memory. So let’s do that by beginning with the concept on slide two, and that concept is overlearning. Basically in overlearning, the idea is that you continue to study something after you can recall it perfectly. So you study some particular topic whatever that topic is. When you can recall it perfectly, you continue to study it. \\\n",
    "This is a classic way to help when one is taking comprehensive finals later in the semester. So when you study for exam one and after you really know it all, you continue to study it. That will make your comprehensive final easier. \\\n",
    "The next factor that will influence memory relates to what we call organization. In general, if you can organize material, you can recall it better. There are lots of different types of organizational strategies and I’ve listed those on slide four. So let’s begin by talking about the first organizational strategy called clustering and is located on page five. \\\n",
    "In clustering, basically you recall items better if you can recognize that there are two or more types of things in a particular list. So let’s give a couple of lists and show you some examples of that. These examples are shown in slide six. \\\n",
    "Let’s say that I give you the first list; north, cardinal, south, robin, east, wren, west, sparrow. Now if you can recognize that north, south, east and west are points on a compass and cardinal, robin, wren and sparrow are birds, then you have a higher probability of recalling that material than if you just tried to recall the list in order. \\\n",
    "The same occurs with the second list that is located on the right hand side of page six. So let’s list these words as well; pig, cat, horse, dog, sheep, birds, cow, and fish. Now if you can recognize that these are two groups of animals; one being farm animals and the other being domestic companions, ala, pets, then you can recall that list of material better than if you just tried to recall the list in order. So again, this is another type of example of organizational strategy. \\\n",
    "Now there are other organizational strategies that one can use as well. The next one of these, as we see on slide seven, are what are called verbal pneumonic techniques. In verbal pneumonic techniques, you make your own organization and there are many, many different types of techniques. So let’s talk about the first of these on slide eight and that is called acrostics. In acrostics these are phrases in which the first letter of each word functions as a cue to help you recall some piece of information. There are a variety of different acrostics that one uses. The most famous of these relates to this saying: On Old Olympus Towering Tops A Fin And German Vented Some Hops. These relate to the twelve different cranial nerves that we have within the brain and if you are a traditional medical student or taking anatomy and physiology, this is the acrostic that you usually use to remember them. \\\n",
    "Now there are other verbal pneumonic techniques as well. So let’s take a look at another one of those and is located in slide 10. These are called acronyms. Acronyms are basically a word formed out of the first letters of a series of words. A classic example of an acronym system is ROY G BIV which are the first letters of the colors in the visual spectrum. This is the classic acronym that all sensation and perception students and even introductory psych students learn to memorize. Another verbal pneumonic technique is shown on slide 11 and called Rhymes. The classic rhyme is one that you had learned in grade school is I before E except after C. So rhymes are another way to recall and memorize information. \\\n",
    "So now we’ve examined a variety of different verbal pneumonic techniques and how they work. In this next section, we’re going to examine some visual imagery types of methods to organize material. \\\n",
    "The first of these is shown in slide 13 and is called the Method of Loci. Basically it involves taking some kind of an imaginary walk along some familiar path where images that you’re trying to recall are associated with some items or locations along the path. The classic example of where we put material is in your house. So just close your eyes and think about this. What happens when you walk in your back door? What’s the first item that you see. Well the first item that you see is where you put the first piece of information that you want to remember. Let’s say that it is a coat hook, so you hang something on the coat hook. Then you continue on into the kitchen. In the kitchen, what’s the first thing that you see? Well it may be the refrigerator, and so you identify the second item that you’re trying to remember and put it on the refrigerator. Then you open the door in the refrigerator and that’s where you put your third item. And then you close the refrigerator door and you look to your left and there’s the stove. So, you put the next item on one of the burners of the stove and on and on until you have all the items that you are trying to remember located within the kitchen or within the house. \\\n",
    "Then when you’re trying to recall the items during the exam, you begin your walk around the house. So the first thing you think about is what happens when I walk in the back door and lo and behold, there’s the first item I’m trying to recall. Then I go to the refrigerator and there’s the second item. Then I open the door of the refrigerator and there’s the third item and on and on until I have all the different materials that I’m trying to remember put down for my exam. \\\n",
    "Now walking around the house is a good place to use the method of loci, but there are other places that’s even better. The better place to try and place all the information you want to learn is in the location where you’re going to have to recall the material. So sitting in the exam room where you’re going to take the test and putting all those things on different objects within the exam room is a good strategy. Especially if one is trying to memorize lots and lots of different information because each of those places acts as a cue. \\\n",
    "So that’s the first type of visual imagery technique. Now the second type of visual imagery technique is shown on slide 14. This is called the pegword technique. The pegword technique relies on a list of integers. What you do is attach a pegword to each of the numbers with rhymes. The classic example is One rhyming with Bun, Two and Shoe, Three and Tree, Four and Door and on and on. Then as we see in slide 15, when you’re given a list of words to recall, you associate the first word in the list with the peg word. For example you have a word, let’s say you’re trying to recall the word “Bee” and the peg word is bun. Well what you might try to do is visualize a bee eating a great big bun. As a result of that, you make associations. Furthermore, the more outrageous the association, the better the recall is for the particular item. So, let’s say that you might have a frog with shoes on, and a horse knocking down a tree, or whatever it may be to the information that you’re trying to recall. \\\n",
    "So these are the first two techniques (overlearning, organization) that relate to factors that influence memory. What’s the next major factor? Well, the next major factor is shown on slides 16 and 17 and that is the order in which you learn things. If I give you a list of words in a serial learning task or a free recall task, you have better recall for words at the beginning and end of a list but not in the middle of a particular list. That is called the Serial Position Effect. There’s a couple different things you need to note about the serial position effect. First recall at the beginning of the list is what is called the Primacy Effect and recall for the end of the list is called the Recency Effect. The recency effect occurs because you can generally only recall seven plus or minus two items in working or what is also called short term memory. We’ll talk about that in more detail a little bit later. \\\n",
    "Now the serial position effect is shown in the graphical figure on slide 19. As we can see when we start to memorize a list of words, we usually start about 60 to 65% accuracy. In the recency phase, at the last word that we’re trying to recall we have a percent recall rate of about 85 to 90% depending on the study. Note within the middle of the list that you’re trying to look at you only have about a 20% chance of recalling a particular set of items. \\\n",
    "So the serial recall effect is extremely important for how you try to memorize things. For example, if you were studying three chapters for learning, as what we might have here, and you always start with the first chapter in order (so you start with chapter one, then two, then three), you will have fairly good recall for chapter one, you’ll have some decent recall also for chapter three but chapter two you won’t remember at all. So a better way to memorize that material is to change the order in which you’re learning the material. So you start one day with chapters one, two, and three in that order, then you go with two, three, and one; and finally, three, one and two, and on and on. What this does is help you raise the level of the middle section within the recency of effect. So, within the serial learning effect, what you should do is vary the order so you have good recall of information. \\\n",
    "Now there’s another variable that goes along with the serial position effect and is shown on slide 20. It is called the Von Resterhoff Effect. Basically when a word is in the middle of a list that is surprising or funny or dirty, you will usually recall that particular word and some of those around it. So let’s give an example of that in the following slide. \\\n",
    "Look on slide 21 at the list of words. What I’d like you to do is try to recall the words in order. So, take a minute to do that. \\\n",
    "Now that you have memorized and tried to recall the words, look in the middle of the list. Basically what happens is that everybody will recall the word 'intercourse' and usually a couple of words around it. So you might recall elephant and even suitcase. This effect is shown on slide 22. So, we have the same kind of effect that we saw with the serial learning task we began with on earlier slides. But we also see that in the middle of the list, we recall one or two particular words, then we drop off again before we have a recency effect (starting toward words 14 and 15). So again, the Von Resterhoff effect is an extremely important effect. You can use it to your advantage by putting surprising or interesting things in the middle of the list of material that you’re trying to memorize. \\\n",
    "Now the next factor that influences learning and memory is what we call proactive interference or what is also called proactive inhibition. Here is where your past learning will interfere with your ability to recall new material. Let’s give an example, if you learn list A, then you learn list B, and finally you have to recall B. In proactive interference, list A will interfere with your ability to recall list B. A classic example is shown on slide 24. You learn sociology then you learn psychology. Sociology will interfere with your ability to recall the psychology. To help keep it clear we have a little organizational scheme that kind of helps us. That is a classic acronym PABB. Proactive - You learn A, you learn B, then you recall B. \\\n",
    "Now sometimes your past learning will interfere with information retention or sometimes your past learning will help you because you learn to organize it better. We will talk about this in more detail in the next section. \\\n",
    "The next factor that influences memory is shown on slide 26 and that is what is called retroactive interference or retroactive inhibition. Unlike proactive interference, learning new material interferes with your ability to recall old material. That is, you learn list C, then you learn list D, then you have to recall C. Consequently, D will interfere with your ability to recall C. \\\n",
    "So let’s take an example of that. Here you learn psychology, then you learn sociology, and sociology will interfere with your ability to recall the psychology. So let’s take another example and use a more practical example. Let’s say that you’re learning psychology, (ala learning) and you’re going to take a test. You walk into the exam and you have about five minutes before the exam starts. So you take out a newspaper such as the Argonaut, which is our school newspaper and you start to read it. Consequently, the Argonaut will interfere with your ability to recall the psychology. Again we have another acronym and that is RCDC. Retroactive - you learn C, you learn D, then you have to recall the C. \\\n",
    "What are important things for you about proactive and retroactive interference from an applied standpoint. This is shown in slide 29. First of all, don’t take similar courses in the same semester. Take things that are different and that don’t have a lot of overlap. As a result you will recall all of them better. An example might be taking some sociology, some math, biology and computer science rather than taking sociology, psychology, anthropology, and maybe political science. If you do, things just get jumbled together. \\\n",
    "Now the next variable that’s going to influence learning is what is called active participation. In general, the more active you are during the learning cycle, the more you will recall, This is shown in slide 31. Quizzing yourself while you’re reading, determining how the material that you’re currently working with relates to other material, using study guides, outlining the chapters or notes, etc., will significantly increase your recall of information. \\\n",
    "The major one these relates to highlighting and chapter outlining or reading. If you look at and see which gives you the better recall, there is no doubt about it that outlining your book chapter will give you better recall than highlighting or reading the book. Let’s just take the concept of highlighting. What are you doing when you highlight a piece of text, let’s say a paragraph or two. What are you doing when you’re doing that? Essentially, what you’re trying to do is keep the yellow or pink line going over the text. You are not really using the information or putting it into your brain system. Whereas if you are outlining some book chapter or some notes, what are you having to do. First, you have to read it, then you have to put it into some kind of verbal vocabulary. Once you have the verbal vocabulary, you have to write it on paper and make sure that it makes sense as you’re doing that. So, when you outline a chapter, you’re putting information into your brain four or five different ways, rather than putting it in and using one or two ways, such as with highlighting or even reading. \\\n",
    "Now the next variable that will impact learning is the similarity of the learning and recall condition. In general, the more similar the recall condition is to the learning condition, the better the recall. This is a classic example that is shown in slide 33. The ideal place to study for an exam is where, the room where you’re going to take the exam. Here you have all the cues. And the more similar it is, (everybody studying in the same room) the more information you recall. \\\n",
    "Now there’s a related concept that goes with the similarity of learning and recall condition and this is shown in slide 35. It is called state dependent learning. What state dependent learning basically says is this. “It’s best to recall information in the same drug state as you are when you’re doing the learning.” So, as we see on slide 36, if you smoke, you need to smoke while taking your exam. If you drink coffee or coke while you’re studying, you need to drink coffee or coke while taking the exam. \\\n",
    "When I was in grad school many, many years ago, I drank a lot of Coca Cola, but I knew exactly how my body felt when I was taking the exam and when I was drinking the Coke while studying. If my Coca Cola level was off, I did poorer on the exam. \\\n",
    "Now a related variable is if you don’t study while drinking coffee, but take the exam on coffee, what happens? Well what happens is that you don’t recall as well (and the same is true with smoking). Since people aren’t allowed to smoke in auditoriums or wherever they are taking exams, it’s best not to smoke when you’re studying. \\\n",
    "Now this concept relates to a concept that is called test anxiety and in test anxiety, what you’re doing is something very similar, and this is shown in slide 37. In test anxiety, basically while you’re studying you tend to be relaxed, but when you’re taking the exam, you tend to get tense due to the stress of the exam. When you’re tense, what happens? Your blood pressure goes up, different hormones are released, etc. As a result, your mind goes “poof” and everything’s gone. Then what happens when you get done with the exam. You walk out, you begin to relax, and guess what happens, you can recall the information again. \\\n",
    "So, the best way to help yourself is to learn to stay relaxed while you’re taking your exam. If you have problems doing that, participate in a test anxiety workshop. There are a variety of those located at a variety of different settings. Furthermore, any good clinical or counseling psychologist can help you with that. \\\n",
    "Now the next variable that relates to factors that influence learning and memory relates to spaced practice being better than massed practice (or what is called cramming). This is shown on slide 39. In general, it’s better to spread out studying over a period of time instead of doing it all at once. Let’s give an example of that on slide 40. Basically studying three days for one hour is better than studying three hours all at once. That is, don’t cram for the exam. The question then becomes why? As we show in slide 41, the reason you have problems is because of the serial position curve. Generally you can only recall seven plus or minus two items in your memory, so when you’re cramming, basically what you’re doing is putting in information into your short term memory. Thus what you have is recalling of recency effect items. \\\n",
    "So in summary as we see here in the last few minutes is that there’s a variety of different factors that influence memory. Each of these factors is extremely important and ones that you should remember. In the next section, we’re going to begin to examine some early theories of memory and how those theories work.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "834b7eda-bc00-4c50-ae0a-a0642b8e9f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = \"in the last section we examined some early aspects of memory in this section what were going to do is discuss some factors that influence memory so lets do that by beginning with the concept on slide two and that concept is overlearning basically in overlearning the idea is that you continue to study something after you can recall it perfectly so you study some particular topic whatever that topic is when you can recall it perfectly you continue to study it \\\n",
    "this is a classic way to help when one is taking comprehensive finals later in the semester so when you study for exam one and after you really know it all you continue to study it that will make your comprehensive final easier \\\n",
    "the next factor that will influence memory relates to what we call organization in general if you can organize material you can recall it better there are lots of different types of organizational strategies and ive listed those on slide four so lets begin by talking about the first organizational strategy called clustering and is located on page five \\\n",
    "in clustering basically you recall items better if you can recognize that there are two or more types of things in a particular list so lets give a couple of lists and show you some examples of that these examples are shown in slide six \\\n",
    "lets say that i give you the first list north cardinal south robin east wren west sparrow now if you can recognize that north south east and west are points on a compass and cardinal robin wren and sparrow are birds then you have a higher probability of recalling that material than if you just tried to recall the list in order \\\n",
    "the same occurs with the second list that is located on the right hand side of page six so lets list these words as well pig cat horse dog sheep birds cow and fish now if you can recognize that these are two groups of animals one being farm animals and the other being domestic companions ala pets then you can recall that list of material better than if you just tried to recall the list in order so again this is another type of example of organizational strategy \\\n",
    "now there are other organizational strategies that one can use as well the next one of these as we see on slide seven are what are called verbal pneumonic techniques in verbal pneumonic techniques you make your own organization and there are many many different types of techniques so lets talk about the first of these on slide eight and that is called acrostics in acrostics these are phrases in which the first letter of each word functions as a cue to help you recall some piece of information there are a variety of different acrostics that one uses the most famous of these relates to this saying on old olympus towering tops a fin and german vented some hops these relate to the twelve different cranial nerves that we have within the brain and if you are a traditional medical student or taking anatomy and physiology this is the acrostic that you usually use to remember them \\\n",
    "now there are other verbal pneumonic techniques as well so lets take a look at another one of those and is located in slide 10 these are called acronyms acronyms are basically a word formed out of the first letters of a series of words a classic example of an acronym system is roy g biv which are the first letters of the colors in the visual spectrum this is the classic acronym that all sensation and perception students and even introductory psych students learn to memorize another verbal pneumonic technique is shown on slide 11 and called rhymes the classic rhyme is one that you had learned in grade school is i before e except after c so rhymes are another way to recall and memorize information \\\n",
    "so now weve examined a variety of different verbal pneumonic techniques and how they work in this next section were going to examine some visual imagery types of methods to organize material \\\n",
    "the first of these is shown in slide 13 and is called the method of loci basically it involves taking some kind of an imaginary walk along some familiar path where images that youre trying to recall are associated with some items or locations along the path the classic example of where we put material is in your house so just close your eyes and think about this what happens when you walk in your back door what’s the first item that you see well the first item that you see is where you put the first piece of information that you want to remember lets say that it is a coat hook so you hang something on the coat hook then you continue on into the kitchen in the kitchen whats the first thing that you see well it may be the refrigerator and so you identify the second item that youre trying to remember and put it on the refrigerator then you open the door in the refrigerator and thats where you put your third item and then you close the refrigerator door and you look to your left and theres the stove so you put the next item on one of the burners of the stove and on and on until you have all the items that you are trying to remember located within the kitchen or within the house \\\n",
    "then when youre trying to recall the items during the exam you begin your walk around the house so the first thing you think about is what happens when i walk in the back door and lo and behold theres the first item im trying to recall then i go to the refrigerator and theres the second item then i open the door of the refrigerator and theres the third item and on and on until i have all the different materials that im trying to remember put down for my exam \\\n",
    "now walking around the house is a good place to use the method of loci but there are other places thats even better the better place to try and place all the information you want to learn is in the location where youre going to have to recall the material so sitting in the exam room where youre going to take the test and putting all those things on different objects within the exam room is a good strategy especially if one is trying to memorize lots and lots of different information because each of those places acts as a cue \\\n",
    "so thats the first type of visual imagery technique now the second type of visual imagery technique is shown on slide 14 this is called the pegword technique the pegword technique relies on a list of integers what you do is attach a pegword to each \\\n",
    " of the numbers with rhymes the classic example is one rhyming with bun two and shoe three and tree four and door and on and on then as we see in slide 15 when youre given a list of words to recall you associate the first word in the list with the peg word for example you have a word lets say youre trying to recall the word bee and the peg word is bun well what you might try to do is visualize a bee eating a great big bun as a result of that you make associations furthermore the more outrageous the association the better the recall is for the particular item so lets say that you might have a frog with shoes on and a horse knocking down a tree or whatever it may be to the information that youre trying to recall \\\n",
    "so these are the first two techniques overlearning organization that relate to factors that influence memory whats the next major factor well the next major factor is shown on slides 16 and 17 and that is the order in which you learn things if i give you a list of words in a serial learning task or a free recall task you have better recall for words at the beginning and end of a list but not in the middle of a particular list that is called the serial position effect theres a couple different things you need to note about the serial position effect first recall at the beginning of the list is what is called the primacy effect and recall for the end of the list is called the recency effect the recency effect occurs because you can generally only recall seven plus or minus two items in working or what is also called short term memory well talk about that in more detail a little bit later \\\n",
    "now the serial position effect is shown in the graphical figure on slide 19 as we can see when we start to memorize a list of words we usually start about 60 to 65 accuracy in the recency phase at the last word that were trying to recall we have a percent recall rate of about 85 to 90 depending on the study note within the middle of the list that youre trying to look at you only have about a 20 chance of recalling a particular set of items \\\n",
    "so the serial recall effect is extremely important for how you try to memorize things for example if you were studying three chapters for learning as what we might have here and you always start with the first chapter in order so you start with chapter one then two then three you will have fairly good recall for chapter one youll have some decent recall also for chapter three but chapter two you wont remember at all so a better way to memorize that material is to change the order in which youre learning the material so you start one day with chapters one two and three in that order then you go with two three and one and finally three one and two and on and on what this does is help you raise the level of the middle section within the recency of effect so within the serial learning effect what you should do is vary the order so you have good recall of information \\\n",
    "now theres another variable that goes along with the serial position effect and is shown on slide 20 it is called the von resterhoff effect basically when a word is in the middle of a list that is surprising or funny or dirty you will usually recall that particular word and some of those around it so lets give an example of that in the following slide \\\n",
    "look on slide 21 at the list of words what id like you to do is try to recall the words in order so take a minute to do that \\\n",
    "now that you have memorized and tried to recall the words look in the middle of the list basically what happens is that everybody will recall the word intercourse and usually a couple of words around it so you might recall elephant and even suitcase this effect is shown on slide 22 so we have the same kind of effect that we saw with the serial learning task we began with on earlier slides but we also see that in the middle of the list we recall one or two particular words then we drop off again before we have a recency effect starting toward words 14 and 15 so again the von resterhoff effect is an extremely important effect you can use it to your advantage by putting surprising or interesting things in the middle of the list of material that youre trying to memorize \\\n",
    "now the next factor that influences learning and memory is what we call proactive interference or what is also called proactive inhibition here is where your past learning will interfere with your ability to recall new material lets give an example if you learn list a then you learn list b and finally you have to recall b in proactive interference list a will interfere with your ability to recall list b a classic example is shown on slide 24 you learn sociology then you learn psychology sociology will interfere with your ability to recall the psychology to help keep it clear we have a little organizational scheme that kind of helps us that is a classic acronym pabb proactive you learn a you learn b then you recall b \\\n",
    "now sometimes your past learning will interfere with information retention or sometimes your past learning will help you because you learn to organize it better we will talk about this in more detail in the next section \\\n",
    "the next factor that influences memory is shown on slide 26 and that is what is called retroactive interference or retroactive inhibition unlike proactive interference learning new material interferes with your ability to recall old material that is you learn list c then you learn list d then you have to recall c consequently d will interfere with your ability to recall c \\\n",
    "so lets take an example of that here you learn psychology then you learn sociology and sociology will interfere with your ability to recall the psychology so lets take another example and use a more practical example lets say that youre learning psychology ala learning and youre going to take a test you walk into the exam and you have about five minutes before the exam starts so you take out a newspaper such as the argonaut which is our school newspaper and you start to read it consequently the argonaut will interfere with your ability to recall the psychology again we have another acronym and that is rcdc retroactive you learn c you learn d then you have to recall the c \\\n",
    "what are important things for you about proactive and retroactive interference from an applied standpoint this is shown in slide 29 first of all dont take similar courses in the same semester take things that are different and that dont have a lot of overlap as a result you will recall all of them better an example might be taking some sociology some math biology and computer science rather than taking sociology psychology anthropology and maybe political science if you do things just get jumbled together \\\n",
    "now the next variable thats going to influence learning is what is called active participation in general the more active you are during the learning cycle the more you will recall this is shown in slide 31 quizzing yourself while youre reading determining how the material that youre currently working with relates to other material using study guides outlining the chapters or notes etc will significantly increase your recall of information \\\n",
    "the major one these relates to highlighting and chapter outlining or reading if you look at and see which gives you the better recall there is no doubt about it that outlining your book chapter will give you better recall than highlighting or reading the book lets just take the concept of highlighting what are you doing when you highlight a piece of text lets say a paragraph or two what are you doing when youre doing that essentially what youre trying to do is keep the yellow or pink line going over the text you are not really using the information or putting it into your brain system whereas if you are outlining some book chapter or some notes what are you having to do first you have to read it then you have to put it into some kind of verbal vocabulary once you \\\n",
    " have the verbal vocabulary you have to write it on paper and make sure that it makes sense as youre doing that so when you outline a chapter youre putting information into your brain four or five different ways rather than putting it in and using one or two ways such as with highlighting or even reading \\\n",
    "now the next variable that will impact learning is the similarity of the learning and recall condition in general the more similar the recall condition is to the learning condition the better the recall this is a classic example that is shown in slide 33 the ideal place to study for an exam is where the room where youre going to take the exam here you have all the cues and the more similar it is everybody studying in the same room the more information you recall \\\n",
    "now theres a related concept that goes with the similarity of learning and recall condition and this is shown in slide 35 it is called state dependent learning what state dependent learning basically says is this its best to recall information in the same drug state as you are when youre doing the learning so as we see on slide 36 if you smoke you need to smoke while taking your exam if you drink coffee or coke while youre studying you need to drink coffee or coke while taking the exam \\\n",
    "when i was in grad school many many years ago i drank a lot of coca cola but i knew exactly how my body felt when i was taking the exam and when i was drinking the coke while studying if my coca cola level was off i did poorer on the exam \\\n",
    "now a related variable is if you dont study while drinking coffee but take the exam on coffee what happens well what happens is that you dont recall as well and the same is true with smoking since people arent allowed to smoke in auditoriums or wherever they are taking exams its best not to smoke when youre studying \\\n",
    "now this concept relates to a concept that is called test anxiety and in test anxiety what youre doing is something very similar and this is shown in slide 37 in test anxiety basically while youre studying you tend to be relaxed but when youre taking the exam you tend to get tense due to the stress of the exam when youre tense what happens your blood pressure goes up different hormones are released etc as a result your mind goes poof and everythings gone then what happens when you get done with the exam you walk out you begin to relax and guess what happens you can recall the information again \\\n",
    "so the best way to help yourself is to learn to stay relaxed while youre taking your exam if you have problems doing that participate in a test anxiety workshop there are a variety of those located at a variety of different settings furthermore any good clinical or counseling psychologist can help you with that \\\n",
    "now the next variable that relates to factors that influence learning and memory relates to spaced practice being better than massed practice or what is called cramming this is shown on slide 39 in general its better to spread out studying over a period of time instead of doing it all at once lets give an example of that on slide 40 basically studying three days for one hour is better than studying three hours all at once that is dont cram for the exam the question then becomes why as we show in slide 41 the reason you have problems is because of the serial position curve generally you can only recall seven plus or minus two items in your memory so when youre cramming basically what youre doing is putting in information into your short term memory thus what you have is recalling of recency effect items \\\n",
    "so in summary as we see here in the last few minutes is that theres a variety of different factors that influence memory each of these factors is extremely important and ones that you should remember in the next section were going to begin to examine some early theories of memory and how those theories work\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3053c393-db3a-46a7-bb6b-ec70afd65543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the last section, we examined some early aspects of memory. In this section, what we’re going to do is discuss some factors that influence memory. So let’s do that by beginning with the concept on slide two, and that concept is overlearning. Basically in overlearning, the idea is that you continue to study something after you can recall it perfectly. So you study some particular topic whatever that topic is. When you can recall it perfectly, you continue to study it. This is a classic way to help when one is taking comprehensive finals later in the semester. So when you study for exam one and after you really know it all, you continue to study it. That will make your comprehensive final easier. The next factor that will influence memory relates to what we call organization. In general, if you can organize material, you can recall it better. There are lots of different types of organizational strategies and I’ve listed those on slide four. So let’s begin by talking about the first organizational strategy called clustering and is located on page five. In clustering, basically you recall items better if you can recognize that there are two or more types of things in a particular list. So let’s give a couple of lists and show you some examples of that. These examples are shown in slide six. Let’s say that I give you the first list; north, cardinal, south, robin, east, wren, west, sparrow. Now if you can recognize that north, south, east and west are points on a compass and cardinal, robin, wren and sparrow are birds, then you have a higher probability of recalling that material than if you just tried to recall the list in order. The same occurs with the second list that is located on the right hand side of page six. So let’s list these words as well; pig, cat, horse, dog, sheep, birds, cow, and fish. Now if you can recognize that these are two groups of animals; one being farm animals and the other being domestic companions, ala, pets, then you can recall that list of material better than if you just tried to recall the list in order. So again, this is another type of example of organizational strategy. Now there are other organizational strategies that one can use as well. The next one of these, as we see on slide seven, are what are called verbal pneumonic techniques. In verbal pneumonic techniques, you make your own organization and there are many, many different types of techniques. So let’s talk about the first of these on slide eight and that is called acrostics. In acrostics these are phrases in which the first letter of each word functions as a cue to help you recall some piece of information. There are a variety of different acrostics that one uses. The most famous of these relates to this saying: On Old Olympus Towering Tops A Fin And German Vented Some Hops. These relate to the twelve different cranial nerves that we have within the brain and if you are a traditional medical student or taking anatomy and physiology, this is the acrostic that you usually use to remember them. Now there are other verbal pneumonic techniques as well. So let’s take a look at another one of those and is located in slide 10. These are called acronyms. Acronyms are basically a word formed out of the first letters of a series of words. A classic example of an acronym system is ROY G BIV which are the first letters of the colors in the visual spectrum. This is the classic acronym that all sensation and perception students and even introductory psych students learn to memorize. Another verbal pneumonic technique is shown on slide 11 and called Rhymes. The classic rhyme is one that you had learned in grade school is I before E except after C. So rhymes are another way to recall and memorize information. So now we’ve examined a variety of different verbal pneumonic techniques and how they work. In this next section, we’re going to examine some visual imagery types of methods to organize material. The first of these is shown in slide 13 and is called the Method of Loci. Basically it involves taking some kind of an imaginary walk along some familiar path where images that you’re trying to recall are associated with some items or locations along the path. The classic example of where we put material is in your house. So just close your eyes and think about this. What happens when you walk in your back door? What’s the first item that you see. Well the first item that you see is where you put the first piece of information that you want to remember. Let’s say that it is a coat hook, so you hang something on the coat hook. Then you continue on into the kitchen. In the kitchen, what’s the first thing that you see? Well it may be the refrigerator, and so you identify the second item that you’re trying to remember and put it on the refrigerator. Then you open the door in the refrigerator and that’s where you put your third item. And then you close the refrigerator door and you look to your left and there’s the stove. So, you put the next item on one of the burners of the stove and on and on until you have all the items that you are trying to remember located within the kitchen or within the house. Then when you’re trying to recall the items during the exam, you begin your walk around the house. So the first thing you think about is what happens when I walk in the back door and lo and behold, there’s the first item I’m trying to recall. Then I go to the refrigerator and there’s the second item. Then I open the door of the refrigerator and there’s the third item and on and on until I have all the different materials that I’m trying to remember put down for my exam. Now walking around the house is a good place to use the method of loci, but there are other places that’s even better. The better place to try and place all the information you want to learn is in the location where you’re going to have to recall the material. So sitting in the exam room where you’re going to take the test and putting all those things on different objects within the exam room is a good strategy. Especially if one is trying to memorize lots and lots of different information because each of those places acts as a cue. So that’s the first type of visual imagery technique. Now the second type of visual imagery technique is shown on slide 14. This is called the pegword technique. The pegword technique relies on a list of integers. What you do is attach a pegword to each of the numbers with rhymes. The classic example is One rhyming with Bun, Two and Shoe, Three and Tree, Four and Door and on and on. Then as we see in slide 15, when you’re given a list of words to recall, you associate the first word in the list with the peg word. For example you have a word, let’s say you’re trying to recall the word “Bee” and the peg word is bun. Well what you might try to do is visualize a bee eating a great big bun. As a result of that, you make associations. Furthermore, the more outrageous the association, the better the recall is for the particular item. So, let’s say that you might have a frog with shoes on, and a horse knocking down a tree, or whatever it may be to the information that you’re trying to recall. So these are the first two techniques (overlearning, organization) that relate to factors that influence memory. What’s the next major factor? Well, the next major factor is shown on slides 16 and 17 and that is the order in which you learn things. If I give you a list of words in a serial learning task or a free recall task, you have better recall for words at the beginning and end of a list but not in the middle of a particular list. That is called the Serial Position Effect. There’s a couple different things you need to note about the serial position effect. First recall at the beginning of the list is what is called the Primacy Effect and recall for the end of the list is called the Recency Effect. The recency effect occurs because you can generally only recall seven plus or minus two items in working or what is also called short term memory. We’ll talk about that in more detail a little bit later. Now the serial position effect is shown in the graphical figure on slide 19. As we can see when we start to memorize a list of words, we usually start about 60 to 65% accuracy. In the recency phase, at the last word that we’re trying to recall we have a percent recall rate of about 85 to 90% depending on the study. Note within the middle of the list that you’re trying to look at you only have about a 20% chance of recalling a particular set of items. So the serial recall effect is extremely important for how you try to memorize things. For example, if you were studying three chapters for learning, as what we might have here, and you always start with the first chapter in order (so you start with chapter one, then two, then three), you will have fairly good recall for chapter one, you’ll have some decent recall also for chapter three but chapter two you won’t remember at all. So a better way to memorize that material is to change the order in which you’re learning the material. So you start one day with chapters one, two, and three in that order, then you go with two, three, and one; and finally, three, one and two, and on and on. What this does is help you raise the level of the middle section within the recency of effect. So, within the serial learning effect, what you should do is vary the order so you have good recall of information. Now there’s another variable that goes along with the serial position effect and is shown on slide 20. It is called the Von Resterhoff Effect. Basically when a word is in the middle of a list that is surprising or funny or dirty, you will usually recall that particular word and some of those around it. So let’s give an example of that in the following slide. Look on slide 21 at the list of words. What I’d like you to do is try to recall the words in order. So, take a minute to do that. Now that you have memorized and tried to recall the words, look in the middle of the list. Basically what happens is that everybody will recall the word 'intercourse' and usually a couple of words around it. So you might recall elephant and even suitcase. This effect is shown on slide 22. So, we have the same kind of effect that we saw with the serial learning task we began with on earlier slides. But we also see that in the middle of the list, we recall one or two particular words, then we drop off again before we have a recency effect (starting toward words 14 and 15). So again, the Von Resterhoff effect is an extremely important effect. You can use it to your advantage by putting surprising or interesting things in the middle of the list of material that you’re trying to memorize. Now the next factor that influences learning and memory is what we call proactive interference or what is also called proactive inhibition. Here is where your past learning will interfere with your ability to recall new material. Let’s give an example, if you learn list A, then you learn list B, and finally you have to recall B. In proactive interference, list A will interfere with your ability to recall list B. A classic example is shown on slide 24. You learn sociology then you learn psychology. Sociology will interfere with your ability to recall the psychology. To help keep it clear we have a little organizational scheme that kind of helps us. That is a classic acronym PABB. Proactive - You learn A, you learn B, then you recall B. Now sometimes your past learning will interfere with information retention or sometimes your past learning will help you because you learn to organize it better. We will talk about this in more detail in the next section. The next factor that influences memory is shown on slide 26 and that is what is called retroactive interference or retroactive inhibition. Unlike proactive interference, learning new material interferes with your ability to recall old material. That is, you learn list C, then you learn list D, then you have to recall C. Consequently, D will interfere with your ability to recall C. So let’s take an example of that. Here you learn psychology, then you learn sociology, and sociology will interfere with your ability to recall the psychology. So let’s take another example and use a more practical example. Let’s say that you’re learning psychology, (ala learning) and you’re going to take a test. You walk into the exam and you have about five minutes before the exam starts. So you take out a newspaper such as the Argonaut, which is our school newspaper and you start to read it. Consequently, the Argonaut will interfere with your ability to recall the psychology. Again we have another acronym and that is RCDC. Retroactive - you learn C, you learn D, then you have to recall the C. What are important things for you about proactive and retroactive interference from an applied standpoint. This is shown in slide 29. First of all, don’t take similar courses in the same semester. Take things that are different and that don’t have a lot of overlap. As a result you will recall all of them better. An example might be taking some sociology, some math, biology and computer science rather than taking sociology, psychology, anthropology, and maybe political science. If you do, things just get jumbled together. Now the next variable that’s going to influence learning is what is called active participation. In general, the more active you are during the learning cycle, the more you will recall, This is shown in slide 31. Quizzing yourself while you’re reading, determining how the material that you’re currently working with relates to other material, using study guides, outlining the chapters or notes, etc., will significantly increase your recall of information. The major one these relates to highlighting and chapter outlining or reading. If you look at and see which gives you the better recall, there is no doubt about it that outlining your book chapter will give you better recall than highlighting or reading the book. Let’s just take the concept of highlighting. What are you doing when you highlight a piece of text, let’s say a paragraph or two. What are you doing when you’re doing that? Essentially, what you’re trying to do is keep the yellow or pink line going over the text. You are not really using the information or putting it into your brain system. Whereas if you are outlining some book chapter or some notes, what are you having to do. First, you have to read it, then you have to put it into some kind of verbal vocabulary. Once you have the verbal vocabulary, you have to write it on paper and make sure that it makes sense as you’re doing that. So, when you outline a chapter, you’re putting information into your brain four or five different ways, rather than putting it in and using one or two ways, such as with highlighting or even reading. Now the next variable that will impact learning is the similarity of the learning and recall condition. In general, the more similar the recall condition is to the learning condition, the better the recall. This is a classic example that is shown in slide 33. The ideal place to study for an exam is where, the room where you’re going to take the exam. Here you have all the cues. And the more similar it is, (everybody studying in the same room) the more information you recall. Now there’s a related concept that goes with the similarity of learning and recall condition and this is shown in slide 35. It is called state dependent learning. What state dependent learning basically says is this. “It’s best to recall information in the same drug state as you are when you’re doing the learning.” So, as we see on slide 36, if you smoke, you need to smoke while taking your exam. If you drink coffee or coke while you’re studying, you need to drink coffee or coke while taking the exam. When I was in grad school many, many years ago, I drank a lot of Coca Cola, but I knew exactly how my body felt when I was taking the exam and when I was drinking the Coke while studying. If my Coca Cola level was off, I did poorer on the exam. Now a related variable is if you don’t study while drinking coffee, but take the exam on coffee, what happens? Well what happens is that you don’t recall as well (and the same is true with smoking). Since people aren’t allowed to smoke in auditoriums or wherever they are taking exams, it’s best not to smoke when you’re studying. Now this concept relates to a concept that is called test anxiety and in test anxiety, what you’re doing is something very similar, and this is shown in slide 37. In test anxiety, basically while you’re studying you tend to be relaxed, but when you’re taking the exam, you tend to get tense due to the stress of the exam. When you’re tense, what happens? Your blood pressure goes up, different hormones are released, etc. As a result, your mind goes “poof” and everything’s gone. Then what happens when you get done with the exam. You walk out, you begin to relax, and guess what happens, you can recall the information again. So, the best way to help yourself is to learn to stay relaxed while you’re taking your exam. If you have problems doing that, participate in a test anxiety workshop. There are a variety of those located at a variety of different settings. Furthermore, any good clinical or counseling psychologist can help you with that. Now the next variable that relates to factors that influence learning and memory relates to spaced practice being better than massed practice (or what is called cramming). This is shown on slide 39. In general, it’s better to spread out studying over a period of time instead of doing it all at once. Let’s give an example of that on slide 40. Basically studying three days for one hour is better than studying three hours all at once. That is, don’t cram for the exam. The question then becomes why? As we show in slide 41, the reason you have problems is because of the serial position curve. Generally you can only recall seven plus or minus two items in your memory, so when you’re cramming, basically what you’re doing is putting in information into your short term memory. Thus what you have is recalling of recency effect items. So in summary as we see here in the last few minutes is that there’s a variety of different factors that influence memory. Each of these factors is extremely important and ones that you should remember. In the next section, we’re going to begin to examine some early theories of memory and how those theories work.\n",
      "\n",
      "In the last section. We examined some early aspects of memory. In this section, what were going to do is discuss. Some factors that influence memory so lets do that by beginning with the concept on slide two, and that concept is overlearning basically in overlearning. The idea is that you continue to study something after you can recall it perfectly so you study. Some particular topic whatever that topic is when you can recall it perfectly you continue to study it. This is a classic way to help when one is taking comprehensive finals later in the semester, so when you study for exam one, and after you really know it. All you continue to study it that will make your comprehensive final easier. The next factor that will influence memory relates to what we call organization in general. If you can organize material you can recall it better. There are lots of different types of organizational strategies, and ive listed those on slide, four so lets begin by talking about the first organizational strategy called clustering, and is located on page five in clustering basically you recall items better. If you can recognize that there are two or more types of things in a particular list, so lets give a couple of lists and show you. Some examples of that these examples are shown in slide six, lets say that i give you the first list north cardinal south robin east wren west sparrow now, if you can recognize that north south east and west are points on a compass and cardinal robin wren and sparrow are birds. Then you have a higher probability of recalling that material than if you just tried to recall the list in order the same occurs with the second list that is located on the right hand side of page six, so lets list these words as well pig cat horse, dog, sheep, birds, cow and fish now. If you can recognize that these are two groups of animals, one being farm animals, and the other being domestic companions ala pets. Then you can recall that list of material better than if you just tried to recall the list in order so again. This is another type of example of organizational strategy. Now there are other organizational strategies that one can use as well. The next one of these as we see on slide. Seven are what are called verbal pneumonic techniques in verbal pneumonic techniques, you make your own organization, and there are many many different types of techniques, so lets talk about the first of these on slide eight, and that is called acrostics. In acrostics. These are phrases in which the first letter of each word functions as a cue to help you recall. Some piece of information. There are a variety of different acrostics that one uses the most famous of these relates to this saying on old olympus towering tops, a fin and german vented. Some hops these relate to the twelve different cranial nerves that we have within the brain, and if you are a traditional medical student or taking anatomy and physiology. This is the acrostic that you usually use to remember them. Now. There are other verbal pneumonic techniques as well, so lets take a look at another one of those and is located in slide. 10. These are called acronyms. Acronyms are basically a word formed out of the first letters of a series of words. A classic example of an acronym system is roy g biv, which are the first letters of the colors in the visual spectrum. This is the classic acronym that all sensation and perception students, and even introductory psych students learn to memorize. Another. Verbal pneumonic technique is shown on slide, 11 and called rhymes. The classic rhyme is one that you had learned in grade school is i before e. Except after c. So rhymes are another way to recall and memorize information so now. Weve examined a variety of different verbal pneumonic techniques, and how they work in this next section were going to examine some visual imagery. Types of methods to organize material. The first of these is shown in slide, 13, and is called the method of loci basically it involves taking some kind of an imaginary walk along some familiar path where images that youre trying to recall are associated with some items or locations along the path. The classic example of where we put material is in your house, so just close your eyes, and think about this. What happens when you walk in your back door. What’s the first item that you see well. The first item that you see is where you put the first piece of information that you want to remember. Lets say that it is a coat hook. So you hang something on the coat hook. Then you continue on into the kitchen in the kitchen. Whats the first thing that you see well. It may be the refrigerator, and so you identify the second item that youre trying to remember, and put it on the refrigerator. Then you open the door in the refrigerator, and thats where you put your third item, and then you close the refrigerator door, and you look to your left, and theres the stove so you put the next item on one of the burners of the stove, and on, and on until you have all the items that you are trying to remember located within the kitchen, or within the house. Then when youre trying to recall the items during the exam. You begin your walk around the house. So the first thing you think about is what happens when i walk in the back door. And lo. And behold. Theres the first item. Im trying to recall. Then i go to the refrigerator, and theres the second item. Then i open the door of the refrigerator, and theres the third item, and on and on until i have all the different materials that im trying to remember put down for my exam. Now walking around the house is a good place to use the method of loci, but there are other places, thats even better. The better place to try, and place all the information you want to learn is in the location, where youre going to have to recall the material. So sitting in the exam room where youre going to take the test, and putting all those things on different objects within the exam room is a good strategy, especially if one is trying to memorize lots and lots of different information, because each of those places acts as a cue. So thats the first type of visual imagery technique. Now the second type of visual imagery technique is shown on slide. 14. This is called the pegword technique. The pegword technique relies on a list of integers. What you do is attach a pegword to each of the numbers with rhymes. The classic example is one rhyming with bun, two and shoe three and tree four and door, and on and on then as we see in slide 15, when youre given a list of words to recall you associate the first word in the list with the peg word. For example, you have a word lets say youre trying to recall the word bee, and the peg word is bun. Well. What you might try to do is visualize a bee eating. A great big bun as a result of that you make associations. Furthermore, the more outrageous the association the better the recall is for the particular item. So lets say that you might have a frog with shoes on and a horse knocking down a tree, or whatever it may be to the information that youre trying to recall so. These are the first two techniques overlearning organization that relate to factors that influence memory. Whats the next major factor well. The next major factor is shown on slides, 16 and 17, and that is the order in which you learn things. If i give you a list of words in a serial learning task or a free recall task. You have better recall for words at the beginning and end of a list, but not in the middle of a particular list that is called the serial position effect. Theres a couple different things, you need to note about the serial position effect first recall at the beginning of the list is what is called the primacy effect and recall for the end of the list is called the recency effect. The recency effect occurs because you can generally only recall seven plus or minus two items in working, or what is also called short term memory. Well talk about that in more detail a little bit later. Now the serial position effect is shown in the graphical figure on slide 19, as we can see when we start to memorize a list of words. We usually start about 60 to 65. Accuracy in the recency phase at the last word that were trying to recall. We have a percent recall rate of about 85 to 90, depending on the study note within the middle of the list that youre trying to look at you only have about a 20 chance of recalling a particular set of items. So the serial recall effect is extremely important for how you try to memorize things. For example, if you were studying three chapters for learning as what we might have here, and you always start with the first chapter in order, so you start with chapter one. Then two. Then three you will have fairly good recall for chapter one. Youll have some decent recall also for chapter three, but chapter two you wont remember at all so a better way to memorize that material is to change the order in which youre learning the material so you start one day with chapters one two and three in that order, then you go with two three, and one, and finally three one and two, and on, and on what this does is help you raise the level of the middle section within the recency of effect, so within the serial learning effect. What you should do is vary. The order so you have good recall of information. Now. Theres another variable that goes along with the serial position effect, and is shown on slide. 20. It is called the von resterhoff effect basically when a word is in the middle of a list that is surprising or funny or dirty. You will usually recall that particular word, and some of those around it. So lets give an example of that in the following slide look on slide 21 at the list of words. What id like you to do is try to recall the words in order, so take a minute to do that now that you have memorized and tried to recall the words look in the middle of the list. Basically what happens is that everybody will recall the word intercourse, and usually a couple of words around it. So you might recall elephant, and even suitcase. This effect is shown on slide. 22 so we have the same kind of effect that we saw with the serial learning task. We began with on earlier slides, but we also see that in the middle of the list we recall one or two particular words. Then we drop off again before we have a recency effect starting toward words, 14 and 15. So again. The von resterhoff effect is an extremely important effect. You can use it to your advantage by putting surprising or interesting things in the middle of the list of material that youre trying to memorize now. The next factor that influences learning and memory is what we call proactive interference, or what is also called proactive inhibition. Here is where your past learning will interfere with your ability to recall new material, lets give an example, if you learn list a. Then you learn list b, and finally you have to recall b in proactive interference list a will interfere with your ability to recall list. B. A classic example is shown on slide. 24. You learn. Sociology then you learn. Psychology, sociology will interfere with your ability to recall the psychology to help keep it clear. We have a little organizational scheme that kind of helps us that is a classic acronym pabb. Proactive you learn a you learn. B. Then you recall. B. Now sometimes your past learning will interfere with information retention, or sometimes your past learning will help you because you learn to organize it better. We will talk about this in more detail in the next section. The next factor that influences memory is shown on slide 26, and that is what is called retroactive interference or retroactive inhibition. Unlike proactive interference, learning, new material interferes with your ability to recall old material that is you learn list c. Then you learn list. D. Then you have to recall c. Consequently, d, will interfere with your ability to recall c. So lets take an example of that here you learn. Psychology, then you learn, sociology and sociology will interfere with your ability to recall the psychology so lets take another, example, and use a more practical. Example, lets say that youre learning psychology ala learning, and youre going to take a test you walk into the exam, and you have about five minutes before the exam starts. So you take out a newspaper such as the argonaut, which is our school newspaper, and you start to read it. Consequently, the argonaut will interfere with your ability to recall the psychology again. We have another acronym, and that is rcdc retroactive you learn. C you learn. D. Then you have to recall the c. What are important things for you about proactive and retroactive interference from an applied standpoint. This is shown in slide. 29 first of all dont take similar courses in the same semester take things that are different, and that dont have a lot of overlap as a result. You will recall all of them better. An example might be taking some sociology, some math biology and computer science rather than taking sociology psychology, anthropology, and maybe political science. If you do things just get jumbled together. Now the next variable, thats going to influence learning is what is called active participation in general. The more active you are during the learning cycle. The more you will recall this is shown in slide. 31 quizzing yourself, while youre reading determining how the material that youre currently working with relates to other material using study guides outlining the chapters or notes. Etc. Will significantly increase your recall of information. The major one these relates to highlighting and chapter outlining or reading. If you look at and see which gives you the better recall. There is no doubt about it that outlining your book. Chapter will give you better recall than highlighting or reading the book, lets just take the concept of highlighting what are you doing when you highlight a piece of text. Lets say a paragraph or two. What are you doing when youre doing that essentially what youre trying to do is keep the yellow or pink line. Going over the text. You are not really using the information or putting it into your brain system. Whereas if you are outlining some book, chapter, or some notes what are you having to do first you have to read it. Then you have to put it into some kind of verbal vocabulary. Once you have the verbal vocabulary. You have to write it on paper, and make sure that it makes sense as youre doing that so when you outline a chapter youre putting information into your brain, four or five different ways rather than putting it in, and using one or two ways such as with highlighting or even reading now, the next variable that will impact learning is the similarity of the learning and recall condition in general. The more similar. The recall condition is to the learning condition. The better the recall this is a classic example that is shown in slide. 33, the ideal place to study for an exam is where the room where youre going to take the exam. Here you have all the cues and the more similar. It is everybody studying in the same room. The more information you recall now. Theres a related concept that goes with the similarity of learning and recall condition, and this is shown in slide. 35. It is called state dependent learning. What state dependent learning basically says is this, its best to recall information in the same drug state as you are when youre doing the learning so as we see on slide 36. If you smoke you need to smoke, while taking your exam if you drink coffee or coke, while youre studying you need to drink coffee or coke, while taking the exam when i was in grad school. Many many years ago, i drank a lot of coca cola, but i knew exactly how my body felt when i was taking the exam, and when i was drinking the coke, while studying if my coca cola level was off. I did poorer on the exam. Now a related variable is if you dont study while drinking coffee, but take the exam on coffee. What happens well. What happens is that you dont recall as well, and the same is true with smoking since people arent allowed to smoke in auditoriums, or wherever they are taking exams its best not to smoke when youre studying now. This concept relates to a concept that is called test anxiety, and in test anxiety. What youre doing is something very similar, and this is shown in slide. 37 in test anxiety basically, while youre studying you tend to be relaxed, but when youre taking the exam you tend to get tense due to the stress of the exam. When youre tense what happens your blood pressure goes up different hormones are released. Etc. As a result. Your mind goes poof and everythings gone. Then what happens when you get done with the exam you walk out you begin to relax, and guess what happens you can recall the information again. So the best way to help yourself is to learn to stay relaxed. While youre taking your exam. If you have problems doing that participate in a test anxiety workshop. There are a variety of those located at a variety of different settings. Furthermore, any good clinical or counseling. Psychologist can help you with that now the next variable that relates to factors that influence learning and memory relates to spaced practice being better than massed practice, or what is called cramming. This is shown on slide 39 in general, its better to spread out studying over a period of time instead of doing it all at once lets give an example of that on slide 40 basically studying three days for one hour is better than studying three hours. All at once that is dont cram for the exam. The question then becomes why as we show in slide. 41. The reason you have problems is because of the serial position curve generally you can only recall seven plus or minus two items in your memory, so when youre cramming basically what youre doing is putting in information into your short term memory. Thus what you have is recalling of recency effect items so in summary as we see here in the last few minutes is that theres a variety of different factors that influence memory. Each of these factors is extremely important and ones that you should remember in the next section were going to begin to examine some early theories of memory, and how those theories work\n"
     ]
    }
   ],
   "source": [
    "final_text_with_tags = predict_and_apply_tags(text_2, model, word_to_idx, idx_to_tag)\n",
    "print(ori_text_2)\n",
    "print(\"\")\n",
    "print(final_text_with_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29a8fb1c-f8fb-46a2-88fb-84a850f0948d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the state dictionary\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel/model_epoch_3_loss_0_257_val_loss_0_261.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# If you plan to only do inference, switch the model to evaluation mode\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming the model architecture is defined as `PunctuationRestorationModel`\n",
    "model = PunctuationRestorationModel(vocab_size, embedding_dim, output_dim, embedding_matrix)  # Ensure these parameters match your model's configuration\n",
    "\n",
    "# Load the state dictionary\n",
    "model.load_state_dict(torch.load('model/model_epoch_3_loss_0_257_val_loss_0_261.pth'))\n",
    "\n",
    "# If you plan to only do inference, switch the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb52d61-1b7d-453c-b663-230430007683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
